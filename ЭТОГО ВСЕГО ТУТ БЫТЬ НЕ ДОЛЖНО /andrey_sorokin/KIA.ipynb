{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "QsFGkXMMwcX3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nH2KyinoxhE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b76f2b8-d69e-479a-bd4a-173ab6652d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken==0.4.0 in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: langchain==0.0.231 in /usr/local/lib/python3.10/dist-packages (0.0.231)\n",
            "Requirement already satisfied: openai==0.27.8 in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: faiss-cpu==1.7.4 in /usr/local/lib/python3.10/dist-packages (1.7.4)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (4.1.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: pydantic==1.10.8 in /usr/local/lib/python3.10/dist-packages (1.10.8)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.4.0) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.4.0) (2.31.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (0.5.14)\n",
            "Requirement already satisfied: langchainplus-sdk<0.0.21,>=0.0.20 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (0.0.20)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (1.23.5)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (1.2.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (8.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==1.10.8) (4.7.1)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from gspread) (2.17.3)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (0.22.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (4.9)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.231) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.231) (0.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2>=0.9.1->oauth2client) (3.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.231) (2.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (5.3.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.231) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.231) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip  install  tiktoken==0.4.0  langchain==0.0.231 openai==0.27.8 faiss-cpu==1.7.4 gspread oauth2client nltk pydantic==1.10.8"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.docstore.document import Document\n",
        "import requests\n",
        "#database\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "#from langchain.document_loaders import TextLoader\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "import pathlib\n",
        "import subprocess\n",
        "import tempfile\n",
        "import ipywidgets as widgets\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import re\n",
        "import getpass\n",
        "import os\n",
        "import openai\n",
        "import tiktoken\n"
      ],
      "metadata": {
        "id": "wNTJkb02qexK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTliDteupaSa",
        "outputId": "b2c136ff-b9e9-4834-a8f1-027ccfdcba8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai_key = getpass.getpass(\"OpenAI API Key:\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "openai.api_key = openai_key"
      ],
      "metadata": {
        "id": "jSdHAWANqs7C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11e3b499-9bc3-4277-b508-bbc3466fad28"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Work DataBase\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "def create_FAISSdb(source_chunks):\n",
        "  return FAISS.from_documents(source_chunks, OpenAIEmbeddings())\n",
        "\n",
        "def save_FAISSdb(db):\n",
        "  db.save_local(\"/content/drive/My Drive/faiss_vector_db_\" + datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\")) #База будет сохранена к вам на Google Drive\n",
        "  return True\n",
        "\n",
        "def load_FAISSdb(db_name):\n",
        "  return FAISS.load_local(db_name, OpenAIEmbeddings())\n",
        "\n",
        "def merg_FAISSdb():\n",
        "  return True\n",
        "\n",
        "def delete_FAISSdb():\n",
        "  return True"
      ],
      "metadata": {
        "id": "Ta3L6Cf2pnZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_TURBO_16K = \"gpt-3.5-turbo-16k\"\n",
        "MODEL_TURBO_0613 = \"gpt-3.5-turbo-0613\"\n",
        "\n",
        "\n",
        "def num_tokens_from_messages(messages, model=MODEL_TURBO_16K):\n",
        "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    if model == \"gpt-3.5-turbo-0301\":  # note: future models may deviate from this\n",
        "        num_tokens = 0\n",
        "        for message in messages:\n",
        "            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
        "            for key, value in message.items():\n",
        "                num_tokens += len(encoding.encode(value))\n",
        "                if key == \"name\":  # if there's a name, the role is omitted\n",
        "                    num_tokens += -1  # role is always required and always 1 token\n",
        "        num_tokens += 2  # every reply is primed with <im_start>assistant\n",
        "        return num_tokens\n",
        "    else:\n",
        "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\"\"\")\n",
        "\n",
        "def insert_newlines(textstr: str, max_len: int = 170) -> str:\n",
        "    words = textstr.split()\n",
        "    lines = []\n",
        "    current_line = \"\"\n",
        "    for word in words:\n",
        "        if len(current_line + \" \" + word) > max_len:\n",
        "            lines.append(current_line)\n",
        "            current_line = \"\"\n",
        "        current_line += \" \" + word\n",
        "    lines.append(current_line)\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def answer_index(system, topic, search_index, temp = 1, verbose = 0, top_similar_documents = 5):\n",
        "\n",
        "    #Выборка документов по схожести с вопросом\n",
        "    docs = search_index.similarity_search(topic, k=top_similar_documents)\n",
        "    if (verbose): print('\\n ===========================================: ')\n",
        "    message_content = re.sub(r'\\n{2}', ' ', '\\n '.join([f'\\nОтрывок документа №{i+1}\\n=====================' + doc.page_content + '\\n' for i, doc in enumerate(docs)]))\n",
        "    if (verbose): print('message_content :\\n ======================================== \\n', message_content)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system},\n",
        "        {\"role\": \"user\", \"content\": f\"Документ с информацией для ответа пользователю: {message_content}\\n\\nВопрос клиента: \\n{topic}\"}\n",
        "    ]\n",
        "\n",
        "    print(messages)\n",
        "\n",
        "    # example token count from the function defined above\n",
        "    if (verbose): print('\\n ===========================================: ')\n",
        "    if (verbose): print(f\"{num_tokens_from_messages(messages, 'gpt-3.5-turbo-0301')} токенов использовано на вопрос\")\n",
        "\n",
        "    completion = openai.ChatCompletion.create(\n",
        "    model=MODEL_TURBO_16K,\n",
        "    messages=messages,\n",
        "    temperature=temp\n",
        "    )\n",
        "    answer = insert_newlines(completion.choices[0].message.content)\n",
        "    return answer  # возвращает ответ"
      ],
      "metadata": {
        "id": "eAafR4VwqtFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# функция для загрузки документа по ссылке из гугл драйв\n",
        "def load_document_text(url: str) -> str:\n",
        "    # Extract the document ID from the URL\n",
        "    match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)\n",
        "    if match_ is None:\n",
        "        raise ValueError('Invalid Google Docs URL')\n",
        "    doc_id = match_.group(1)\n",
        "\n",
        "    # Download the document as plain text\n",
        "    response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')\n",
        "    response.raise_for_status()\n",
        "    text = response.text\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "1YFsrIhi4Yct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "\n",
        "def load_search_indexes(url: str) -> str:\n",
        "    # Extract the document ID from the URL\n",
        "    match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)\n",
        "    if match_ is None:\n",
        "        raise ValueError('Invalid Google Docs URL')\n",
        "    doc_id = match_.group(1)\n",
        "\n",
        "    # Download the document as plain text\n",
        "    response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')\n",
        "    response.raise_for_status()\n",
        "    #text = response.content.decode('unicode_escape')\n",
        "    text = response.text\n",
        "    headers_to_split_on = [\n",
        "        (\"#\", \"Header 1\"),\n",
        "        (\"##\", \"Header 2\"),\n",
        "        (\"###\", \"Header 3\"),\n",
        "    ]\n",
        "\n",
        "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "    md_header_splits = markdown_splitter.split_text(text)\n",
        "\n",
        "    vectordateBase = FAISS.from_documents(md_header_splits, OpenAIEmbeddings())\n",
        "    save_FAISSdb(vectordateBase)\n",
        "    return vectordateBase\n"
      ],
      "metadata": {
        "id": "hu-w7euIqtNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "db = load_search_indexes('https://docs.google.com/document/d/1koct2pxGaItFv0l4c4sk7X3xAflWz7pwCFp6WlIkXdw')\n",
        "\n",
        "#db = load_FAISSdb('/content/drive/My Drive/faiss_vector_db_08-30-2023-13-19-16')"
      ],
      "metadata": {
        "id": "9CrXhbfVqtP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_user_question_dialog(system: str, db: str, user_question: str, question_history: list) -> str:\n",
        "\n",
        "    # Если в истории более одного вопроса, применяем суммаризацию\n",
        "   # summarized_history = \"\"\n",
        "   # if len(question_history) > 0:\n",
        "    #    summarized_history = \"Вот краткий обзор предыдущего диалога: \" + summarize_questions([q + ' ' + (a if a is not None else '') for q, a in question_history])\n",
        "\n",
        "    # Добавляем явное разделение между историей диалога и текущим вопросом\n",
        "   # input_text =summarized_history + \"\\n\\nТекущий ответ пользователя: \" + user_question\n",
        "\n",
        "    # Извлечение наиболее похожих отрезков текста из базы знаний и получение ответа модели\n",
        "   # answer_text = answer_index(system, input_text, db, temperature, verbose)\n",
        "    answer_text = answer_index(system, user_question, db, temperature, verbose)\n",
        "\n",
        "    # Добавляем вопрос пользователя и ответ системы в историю\n",
        "   # question_history.append((\"\\n Пользователь: \" + user_question, \"\\n Консультант: \" + answer_text if answer_text is not None else ''))\n",
        "\n",
        "    # Выводим суммаризированный текст, который видит модель\n",
        "    #if summarized_history != \"\":\n",
        "    #if 1 == 0 :\n",
        "     #   print('****************************')\n",
        "      #  print(insert_newlines(summarized_history))\n",
        "       # print('****************************')\n",
        "\n",
        "    return insert_newlines(answer_text)"
      ],
      "metadata": {
        "id": "dsEKLyifvEto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_dialog(system_doc_url, knowledge_base_url):\n",
        "    question_history = []\n",
        "    dialog = \"\"\n",
        "    while True:\n",
        "        user_question = input('Клиент: ')\n",
        "        if ((user_question.lower() == 'stop') or (user_question.lower() == 'стоп')):\n",
        "            break\n",
        "        answer = answer_user_question_dialog(system_doc_url, knowledge_base_url, user_question, question_history)\n",
        "        dialog += f'\\nПользователь: {user_question} \\n Менеджер: {answer}'\n",
        "        print('\\nМенеджер: ', answer+'\\n')\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "g8N4WuC7qtKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Промпт моделей машин\n",
        "promt_url = load_document_text ('https://docs.google.com/document/d/1tjjJQ0s0qKSdmg2Yd69NXX4_RGWLr0up')"
      ],
      "metadata": {
        "id": "YCBakZx6qtVM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "d332e488-cade-4687-a024-e508f9ae907e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a588319f56a2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Промпт моделей машин\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpromt_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_document_text\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'https://docs.google.com/document/d/1tjjJQ0s0qKSdmg2Yd69NXX4_RGWLr0up'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-668997ff7b11>\u001b[0m in \u001b[0;36mload_document_text\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_document_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Extract the document ID from the URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmatch_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/document/d/([a-zA-Z0-9-_]+)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmatch_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid Google Docs URL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temperature=0.3\n",
        "verbose=0\n",
        "\n",
        "run_dialog(promt_url, db)"
      ],
      "metadata": {
        "id": "WWY1AS2GvTN5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}