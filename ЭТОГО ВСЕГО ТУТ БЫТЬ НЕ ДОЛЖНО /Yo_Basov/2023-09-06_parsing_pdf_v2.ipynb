{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jioG-OyYNEtz"
      },
      "outputs": [],
      "source": [
        "# инсталлируем библиотеку для работы с pdf\n",
        "\n",
        "!pip install pdfminer.six"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# импортируем нужные модули\n",
        "\n",
        "import pdfminer.high_level\n",
        "import requests\n",
        "import os\n",
        "import re\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "-wV-sx1_np79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Функция №1\n",
        "Эта функция, получает на вход URL-ссылку на PDF файл и переводит его в текст."
      ],
      "metadata": {
        "id": "rHX7NJubThSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Эта функция принимает на вход URL-адрес\n",
        "по которому находится pdf файл, переводит его в текстовый формат\n",
        "и сохраняет на диск.\n",
        "\n",
        "  Вход:\n",
        "    url - адрес PDF файла\n",
        "  Выход:\n",
        "    topic - текст\n",
        "'''\n",
        "\n",
        "def pdf_to_txt(url):\n",
        "  topic = ''\n",
        "  pattern = re.compile(r'\\w[а-я]')\n",
        "  # получаем файл\n",
        "  pdf_req = requests.get(url)\n",
        "  # проверяем, что страница удачно парсится\n",
        "  if pdf_req.status_code == 200:\n",
        "    # запоминаем название файла без расширения\n",
        "    f_name = os.path.basename(url).split('.')[0]\n",
        "    # записываем файл на диск\n",
        "    with open(f'{f_name}.pdf','wb') as f:\n",
        "      f.write(pdf_req.content)\n",
        "    # переводим файл в текст\n",
        "    text = pdfminer.high_level.extract_text(f'{f_name}.pdf')\n",
        "    # удаляем pdf файл\n",
        "    if os.path.isfile(f'{f_name}.pdf'):\n",
        "      os.remove(f'{f_name}.pdf')\n",
        "    # исправляем некоторые ошибки парсинга\n",
        "    text = text.replace(' \\n', ' ')\n",
        "    text = text.replace('-\\n', '-')\n",
        "    # разбиваем текст на строки для дальнейшей обработки\n",
        "    txt_split = text.split('\\n')\n",
        "    # убираем пробелы в начале и конце каждой строки\n",
        "    for st in range(len(txt_split)):\n",
        "      res = txt_split[st].strip()\n",
        "      # убираем повторяющиеся пробелы внутри строки\n",
        "      res = re.sub('\\s+',' ', res)\n",
        "      # если строка не пустая и не номер страницы - добавляем её к тексту\n",
        "      if res and not res.isdigit() and (len(res)>1):\n",
        "        # это условие убирает ошибочные разрывы абзаца\n",
        "        if re.match(r'[а-я]', res[0]):\n",
        "          topic = topic[:-1]\n",
        "          topic += ' '\n",
        "        topic += res+'\\n'\n",
        "    # записываем текст в файл с названием оригинала, но txt\n",
        "    with open(f'{f_name}.txt','w') as f:\n",
        "      f.write(topic)\n",
        "\n",
        "    return topic\n"
      ],
      "metadata": {
        "id": "-9lJ4C2a69Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# пррверяем как работает функция на конкретном файле\n",
        "\n",
        "url = 'https://cdn.kia.ru/master-data/brochures/Kia_Corporate_Sales.pdf'\n",
        "\n",
        "print(pdf_to_txt(url))"
      ],
      "metadata": {
        "id": "gm9ecJRZDtwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Функция №2\n",
        "Эта функция, получает на вход URL-ссылку на страницу сайта и выбирает из неё все ссылки на PDF файлы. Возвращает в виде словаря."
      ],
      "metadata": {
        "id": "heXsmjWlTuJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_pdf(url):\n",
        "  # Получаем запрос от страницы\n",
        "  response = requests.get(url)\n",
        "  # Получаем ПрекрасныйСуп\n",
        "  bs = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "  # Определяем функцию отбора тегов. Берём теги у которых есть ссылка.\n",
        "  def is_href(href):\n",
        "    return href\n",
        "\n",
        "  # Отбитаем теги у которых есть ссылка\n",
        "  bs.find_all(href = is_href)\n",
        "\n",
        "  # Заводим пустой словарь\n",
        "  PDF_name = {}\n",
        "\n",
        "  # Наполняем словарь описаниями файла и ссылками\n",
        "  # То, что это словарь - важно. Так мы избегаем одинаковых файлов, но с разными ссылками. (такое есть)\n",
        "  for tag in bs.find_all(href = is_href):\n",
        "      # Извлекаем ссылку из тега\n",
        "      tag_href = tag.attrs['href']\n",
        "      # Делаем условие, если ссылка на файл, который оканчивается на .pdf\n",
        "      if tag_href.split('.')[-1] == 'pdf':\n",
        "        # Добавляем новый элемент в словарь\n",
        "        PDF_name[tag.get_text().strip()] = tag_href\n",
        "  # Возвращаем словарь\n",
        "  return PDF_name"
      ],
      "metadata": {
        "id": "jiohytOOHvCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Смотрим как работает функция на примере одного URL."
      ],
      "metadata": {
        "id": "YIfTXQOfXsgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Смотрим как работает функция\n",
        "\n",
        "# Берём произвольную страницу из БЗ\n",
        "url = 'https://www.kia.ru/auction/'\n",
        "\n",
        "# Делаем запрос и извлекаем название страницы\n",
        "response = requests.get(url)\n",
        "bs = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Извлекаем \"словарь\" (список PDFок с сайта)\n",
        "PDFs = find_pdf(url)\n",
        "\n",
        "# Условие: Что-то печаем, если PDF на странице сайта есть\n",
        "if PDFs:\n",
        "  # Если PDFки есть, то то печатаем заголовок\n",
        "  print(f'Страница: {bs.title.string}')\n",
        "  print(f'Адрес страницы: {url}')\n",
        "\n",
        "  # Распечатываем PDF файлы, которые есть на этой странице\n",
        "  for key,value in PDFs.items():\n",
        "    print(f'PDF - \"{key}\"\\n\\tссылка: {value}')\n"
      ],
      "metadata": {
        "id": "bhZtoXR2K6Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Функция №3\n",
        "А теперь проходимся по всей базе и находим все PDFки.\n",
        "Сначала подгружаем БД:\n",
        "\n",
        "\n",
        "Поскольку я начал в парсинге заниматься PDF, то могу сказать, что в PDF информации даже чуть больше, чем на страницах.\n",
        "Цикл проходтися по всем нашим ссылкам и выдирает все PDFки."
      ],
      "metadata": {
        "id": "WrAOs5U7X8Ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cкачиваем фалы данных в виде csv\n",
        "\n",
        "def load_document_csv(url: str) -> str:\n",
        "    # Extract the document ID from the URL\n",
        "    match_ = re.search('/spreadsheets/d/([a-zA-Z0-9-_]+)', url)\n",
        "    if match_ is None:\n",
        "        raise ValueError('Invalid Google Docs URL')\n",
        "    doc_id = match_.group(1)\n",
        "\n",
        "    # Download the document as plain text\n",
        "    response = requests.get(f'https://docs.google.com/spreadsheets/d/{doc_id}/export?format=tsv')\n",
        "    response.raise_for_status()\n",
        "    text = response.content\n",
        "\n",
        "    return text\n",
        "\n",
        "urls = load_document_csv('https://docs.google.com/spreadsheets/d/1h5BNBTXwzb8nMTIcre0vCDcujax1Fvfw3RdwT7FeAt8/edit?usp=sharing')\n",
        "with open('urls_r.csv','wb') as f:\n",
        "  f.write(urls)"
      ],
      "metadata": {
        "id": "pPlj5k5q5XDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Дальше загружаем из файла данных в массим Numpy\n",
        "\n",
        "links = np.genfromtxt('urls_r.csv', delimiter='\\t', dtype=str, encoding='utf-8' )"
      ],
      "metadata": {
        "id": "QkIRH7059reJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Идём по списку ссылок\n",
        "for row in links:\n",
        "\n",
        "  # Делаем запрос и извлекаем URL стреницы, который находится в столбце [1]\n",
        "  response = requests.get(row[1])\n",
        "  bs = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "  # Ищем все вхождения PDF (описание функции выше)\n",
        "  PDFs = find_pdf(row[1])\n",
        "\n",
        "  # Условие: Что-то печатаем, если PDF на странице сайта есть\n",
        "  if PDFs:\n",
        "    # Если PDFки есть, то печатаем заголовок\n",
        "    print(f'\\nСтраница: {bs.title.string}')\n",
        "    print(f'Адрес страницы: {row[1]}')\n",
        "\n",
        "    # Распечатываем PDF файлы, которые встречаются на страницах\n",
        "    for key,value in PDFs.items():\n",
        "      print(f'PDF - \"{key}\"\\n\\tссылка: {value}')\n"
      ],
      "metadata": {
        "id": "2vjYTJNec6gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "И теперь мы можем преобразовать все PDF файлы в текст при помощи Функции №1. Разделить его на чанки любым из доступных способов, а также придумать свой, оригинальный. ))"
      ],
      "metadata": {
        "id": "fJ4HmBQ-Bex_"
      }
    }
  ]
}