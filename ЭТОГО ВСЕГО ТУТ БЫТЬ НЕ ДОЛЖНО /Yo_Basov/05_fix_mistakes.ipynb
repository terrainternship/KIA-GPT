{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ЧЕРНОВОЙ ВАРИАНТ ПО САММАРИЗАЦИИ ВИДЕО БЕЗ ТИТРОВ В COLAB c ПРОВЕРКОЙ НА ДУБЛИ и ОПЕЧАТКИ\n",
        "\n",
        "# Можете копию сделать, поиграться, только на Google Disk надо в KIA_VIDEO папке CURRENT.docx файл создать, дать права на просмотр и ссылку подставить вместо моей в переменную CURRENT=.\n",
        "\n",
        "# # Для использования на гугл диске в меню доступные мне найдите V1 и замонтируйте как папку к себе на Мой Диск\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "S62SU3VqBsrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain openai chromadb"
      ],
      "metadata": {
        "id": "oRkRECFvBpI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "    (\"###\", \"Header 3\"),\n",
        "]\n",
        "\n",
        "name = \"\"\n",
        "db_splits = []\n",
        "markdown_document = \"\"\n",
        "with codecs.open(\"/content/drive/My Drive/V1/KIA (Транскрибация)/parser.txt\", \"r\", encoding=encoding) as fs:\n",
        "    while True:\n",
        "        fs_line = fs.readline().encode('utf-8', 'ignore').decode(encoding)\n",
        "        if not fs_line: break\n",
        "        name = re.match('<[^>]*>', fs_line)\n",
        "        name = name.encode(encoding, 'ignore').decode('utf-8') if name else \"\"\n",
        "        markdown_document += re.sub('<[^>]*>', '', fs_line).encode(encoding, 'ignore').decode('utf-8')\n",
        "# MD splits\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
        "\n",
        "# Char-level splits\n",
        "chunk_size = 1024\n",
        "chunk_overlap = 30\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        ")\n",
        "\n",
        "# Split\n",
        "splits = text_splitter.split_documents(md_header_splits)\n",
        "db_splits = [split.page_content for split in splits]"
      ],
      "metadata": {
        "id": "fYZYWqGuBWyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p \"/content/drive/My Drive/KIA_VIDEO/\""
      ],
      "metadata": {
        "id": "Dy_ibKOXIOve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WHISPER_TEXTS={\"11.2.3\" : [\"https://drive.google.com/file/d/\", \"1KY134ZBEkitRkrpdfH_ISaef_uoUaMMu\", \"/view?usp=sharing\"]}"
      ],
      "metadata": {
        "id": "nnbRyE8t-OLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MY_FIXES=\"\"\"киос кэ\tкиоске\n",
        "тридын\tтрейд ин\n",
        "кеоптима\tкиа оптима\n",
        "марк укея\tмарку киа\n",
        "кеа оптима\tкиа оптима\n",
        "кеа\tкиа\n",
        "кестингер\tкиа стингер\"\"\"\n",
        "MY_PROMPT=\"\"\"\n",
        "У тебя есть подробная информация по работе с ним.\n",
        "Тебе задает вопрос пользователь, дай ему информацию, опираясь на предоставленные материалы.\n",
        "Отвечай максимально точно и используй только информацию из документов, не добавляй ничего своего.\n",
        "Описывай текст словами клиента. Ты должен дать ответ в форме рекомендации. Cтарайся использовать безличные предложения. Удали из своего ответа редакторов, авторов. Не называй в ответе фамилий и имен.\n",
        "Предварительно исправь в документе все опечатки: \"\"\" + \",\".join([\" замени \\\"\" + line.split(\"\\t\")[0] + \"\\\" на \\\"\" + line.split(\"\\t\")[1] + \"\\\"\" for line in MY_FIXES.split(\"\\n\")]) + \"\"\".\n",
        "Документ с информацией для ответа пользователю:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "pG9wvU2--N-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGw8-HR7ZTPf"
      },
      "outputs": [],
      "source": [
        "#@title Установка библиотек\n",
        "!pip  install  tiktoken  langchain openai chromadb gspread oauth2client nltk pydantic==1.10.8"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "id": "baaz9Qm4Iyxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Импорт библиотек\n",
        "import gdown\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.docstore.document import Document\n",
        "import requests\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "import pathlib\n",
        "import subprocess\n",
        "import tempfile\n",
        "import ipywidgets as widgets\n",
        "import os\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import re\n",
        "import getpass\n",
        "import os\n",
        "import openai\n",
        "import tiktoken"
      ],
      "metadata": {
        "id": "s2dJmIzdel-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Получение ключа API от пользователя и установка его как переменной окружения\n",
        "openai_key = getpass.getpass(\"OpenAI API Key:\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "openai.api_key = openai_key"
      ],
      "metadata": {
        "id": "ms72WfoYemBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Переводим текстовый файл в word\n",
        "import requests\n",
        "import docx\n",
        "\n",
        "def convert_txt_to_docx(file_ids, output_path):\n",
        "    response = requests.get(\"https://docs.google.com/uc?export=download&id=\"+file_ids[1])\n",
        "    encoding = response.encoding\n",
        "    content = response.text\n",
        "\n",
        "    doc = docx.Document()\n",
        "    doc.add_paragraph(content.encode(encoding, 'ignore').decode('utf-8'))\n",
        "\n",
        "    doc.save(output_path)\n",
        "\n",
        "    print(encoding)"
      ],
      "metadata": {
        "id": "uvVFQGyrAQ8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Монтируем каталог на гугл диске и в нем файлу CURRENT.docx даем права на чтение всем, у кого ссылка\n",
        "convert_txt_to_docx(WHISPER_TEXTS[\"11.2.3\"], \"/content/drive/My Drive/KIA_VIDEO/CURRENT.docx\")\n",
        "CURRENT=\"https://docs.google.com/document/d/1EMnFE4K4cKzbFAh9fKYSUiHJRm4ib9ao/edit\""
      ],
      "metadata": {
        "id": "9y9hwZSCAX_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Нарезаем CURRENT.docx на чанки\n",
        "def split_text(text, max_count, count_type, verbose=0):\n",
        "    # Функция для подсчета количества слов в фрагменте\n",
        "    def num_words(fragment):\n",
        "        return len(fragment.split())\n",
        "\n",
        "    # Функция для подсчета количества токенов в фрагменте\n",
        "    def num_tokens(fragment):\n",
        "        return num_tokens_from_string(fragment, \"cl100k_base\")\n",
        "\n",
        "    # Разделение текста на фрагменты, исключая теги HTML\n",
        "    fragments = [fragment.strip() for fragment in re.split(r\"<[^>]+>|[\\ufeff]\", text) if fragment.strip()]\n",
        "\n",
        "    # Выбор функции подсчета длины в зависимости от типа подсчета\n",
        "    length_function = num_words if count_type == \"words\" else num_tokens\n",
        "\n",
        "    # Создание объекта разделителя текста\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=max_count, chunk_overlap=0, length_function=length_function)\n",
        "\n",
        "    # НОВОЕ - фильтруем очень похожие чанки, если они был в базе parser.txt\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # Список для хранения фрагментов текста\n",
        "    source_chunks = []\n",
        "\n",
        "    # Обработка каждого фрагмента текста\n",
        "    for fragment in fragments:\n",
        "        if verbose:\n",
        "            # Вывод количества слов/токенов в фрагменте, если включен режим verbose\n",
        "            count = length_function(fragment)\n",
        "            print(f\"{count_type} in text fragment = {count}\\n{'-' * 5}\\n{fragment}\\n{'=' * 20}\")\n",
        "\n",
        "        # Разбиение фрагмента текста на части заданной длины с помощью разделителя\n",
        "        # и добавление каждой части в список source_chunks\n",
        "        source_chunks.extend(Document(page_content=chunk, metadata={}) for chunk in splitter.split_text(fragment))\n",
        "\n",
        "    # Возвращение списка фрагментов текста\n",
        "    return source_chunks\n",
        "\n",
        "\n",
        "def create_embedding(data, max_count, count_type):\n",
        "    def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "      \"\"\"Возвращает количество токенов в строке\"\"\"\n",
        "      encoding = tiktoken.get_encoding(encoding_name)\n",
        "      num_tokens = len(encoding.encode(string))\n",
        "      return num_tokens\n",
        "\n",
        "    source_chunks = []\n",
        "\n",
        "    source_chunks = split_text(text=data, max_count=max_count, count_type=count_type, verbose=0)\n",
        "\n",
        "    # Создание индексов документа\n",
        "    search_index = Chroma.from_documents(source_chunks, OpenAIEmbeddings(), )\n",
        "\n",
        "    count_token = num_tokens_from_string(' '.join([x.page_content for x in source_chunks]), \"cl100k_base\")\n",
        "    print('\\n ===========================================: ')\n",
        "    print('Количество токенов в документе :', count_token)\n",
        "    print('ЦЕНА запроса:', 0.0004*(count_token/1000), ' $')\n",
        "    return search_index\n",
        "\n",
        "def load_search_indexes(url: str, max_count, count_type) -> str:\n",
        "    # Extract the document ID from the URL\n",
        "    match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)\n",
        "    if match_ is None:\n",
        "        raise ValueError('Invalid Google Docs URL')\n",
        "    doc_id = match_.group(1)\n",
        "\n",
        "    # Download the document as plain text\n",
        "    response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')\n",
        "    response.raise_for_status()\n",
        "    text = response.text\n",
        "    return create_embedding(text, max_count=max_count, count_type=count_type)\n"
      ],
      "metadata": {
        "id": "mRr4BhXnAuup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Готовим для CURRENT.docx эмбединги\n",
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
        "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    if model == \"gpt-3.5-turbo-0301\":  # note: future models may deviate from this\n",
        "        num_tokens = 0\n",
        "        for message in messages:\n",
        "            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
        "            for key, value in message.items():\n",
        "                num_tokens += len(encoding.encode(value))\n",
        "                if key == \"name\":  # if there's a name, the role is omitted\n",
        "                    num_tokens += -1  # role is always required and always 1 token\n",
        "        num_tokens += 2  # every reply is primed with <im_start>assistant\n",
        "        return num_tokens\n",
        "    else:\n",
        "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\"\"\")\n",
        "\n",
        "def insert_newlines(text: str, max_len: int = 170) -> str:\n",
        "    words = text.split()\n",
        "    lines = []\n",
        "    current_line = \"\"\n",
        "    for word in words:\n",
        "        if len(current_line + \" \" + word) > max_len:\n",
        "            lines.append(current_line)\n",
        "            current_line = \"\"\n",
        "        current_line += \" \" + word\n",
        "    lines.append(current_line)\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def answer_index(system, topic, search_index, temp = 1, verbose = 0, top_similar_documents = 5):\n",
        "\n",
        "    #Выборка документов по схожести с вопросом\n",
        "    docs = search_index.similarity_search(topic, k=top_similar_documents)\n",
        "    if (verbose): print('\\n ===========================================: ')\n",
        "    message_content = re.sub(r'\\n{2}', ' ', '\\n '.join([f'\\nОтрывок документа №{i+1}\\n=====================' + doc.page_content + '\\n' for i, doc in enumerate(docs)]))\n",
        "    if (verbose): print('message_content :\\n ======================================== \\n', message_content)\n",
        "\n",
        "    messages = [\n",
        "      {\"role\": \"system\", \"content\": system + f\"{message_content}\"},\n",
        "      {\"role\": \"user\", \"content\": topic}\n",
        "      ]\n",
        "\n",
        "    # example token count from the function defined above\n",
        "    if (verbose): print('\\n ===========================================: ')\n",
        "    if (verbose): print(f\"{num_tokens_from_messages(messages, 'gpt-3.5-turbo-0301')} токенов использовано на вопрос\")\n",
        "\n",
        "    completion = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    temperature=temp\n",
        "    )\n",
        "    if (verbose): print('\\n ===========================================: ')\n",
        "    if (verbose): print(f'{completion[\"usage\"][\"total_tokens\"]} токенов использовано всего (вопрос-ответ).')\n",
        "    if (verbose): print('\\n ===========================================: ')\n",
        "    if (verbose): print('ЦЕНА запроса с ответом :', 0.002*(completion[\"usage\"][\"total_tokens\"]/1000), ' $')\n",
        "    if (verbose): print('\\n ===========================================: ')\n",
        "\n",
        "    res = insert_newlines(completion.choices[0].message.content)\n",
        "    print('ОТВЕТ : \\n', )\n",
        "\n",
        "    return res\n",
        "    # return completion"
      ],
      "metadata": {
        "id": "pOc-jkz7A1F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Просим ChatGPT ответить на вопрос \"Кратко перескажи текст:\"\n",
        "max_count = 110\n",
        "count_type = \"words\"\n",
        "top_similar_documents = 5\n",
        "# Документ \"инструкция по эксплуатации\"\n",
        "manual_index = load_search_indexes(CURRENT, max_count=max_count, count_type=count_type)\n",
        "ans=answer_index(\n",
        "    MY_PROMPT,\n",
        "    'Кратко перескажи текст.',                                              # напишите вопрос для проверки инструкции\n",
        "    manual_index,\n",
        "    verbose = 1,\n",
        "    top_similar_documents = top_similar_documents\n",
        ")"
      ],
      "metadata": {
        "id": "eENSwfgpA5i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохраняем ответ на гугл диск\n",
        "def save_ans_to_docx(ans, output_path):\n",
        "    content = ans\n",
        "\n",
        "    doc = docx.Document()\n",
        "    doc.add_paragraph(content)\n",
        "    doc.save(output_path)\n",
        "save_ans_to_docx(ans, \"/content/drive/My Drive/V1/KIA (Транскрибация)/\" + \"_11.2.3\" + \".docx\")"
      ],
      "metadata": {
        "id": "g3XqIi-qOwct"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}