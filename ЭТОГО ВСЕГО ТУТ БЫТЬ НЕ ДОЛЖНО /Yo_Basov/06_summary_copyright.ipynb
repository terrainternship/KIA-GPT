{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ЧЕРНОВОЙ ВАРИАНТ ПО НЕЙРО-КОПИРАЙТЕРУ ВИДЕО БЕЗ ТИТРОВ В COLAB\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "S62SU3VqBsrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf \"/content/drive/MyDrive/KIA_VIDEO/Copywriter/KIA/BASE/\"\n",
        "!mkdir -p \"/content/drive/MyDrive/KIA_VIDEO/Copywriter/KIA/BASE/\""
      ],
      "metadata": {
        "id": "Dy_ibKOXIOve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HOWTO_TEXTS={#\"12.1.1\" : \"https://drive.google.com/file/d/10P6JttipRmhyczrHIedjgsHBE2iX2KXd/view?usp=drive_link\"\n",
        "#, \"12.1.2\": \"https://drive.google.com/file/d/1kCgDeK6dsj7FEH5iRDM1wqnciZ9r87-g/view?usp=drive_link\"\n",
        "#, \"12.1.3\": \"https://drive.google.com/file/d/1_kKtp6rJp911mtljJ0VzwdL0YeTVBApn/view?usp=drive_link\"\n",
        "#, \"12.1.4\": \"https://drive.google.com/file/d/1kLOKaTwZ4Hc0_a46WgsD-foDbnBsv6KG/view?usp=drive_link\"\n",
        "#, \"12.2.1\": \"https://drive.google.com/file/d/15bbALOsgazmk1U7aE5se1n5rE5Rw2Tu8/view?usp=drive_link\"\n",
        "#, \"12.2.2\": \"https://drive.google.com/file/d/1Kgx_PFmDy0aqg-6zB9rLcCUxTE5W4ujJ/view?usp=drive_link\"\n",
        "#, \"12.2.3\": \"https://drive.google.com/file/d/166PSqkusw9dBZEJKm9nHiLhGpmKNoGw0/view?usp=drive_link\"\n",
        "#, \"12.2.4\": \"https://drive.google.com/file/d/1IoEX-FZ_7UXX2fWj0g7OnW9CbIlgFyBy/view?usp=drive_link\"\n",
        "#, \"12.2.5\": \"https://drive.google.com/file/d/1leq5RhhriIcIDy6m9cKDWcs41bOSlrD4/view?usp=drive_link\"\n",
        "#, \"12.2.6\": \"https://drive.google.com/file/d/1FlpE0IdQZb-XGt-j6nxi4GjmnD8V3Bjv/view?usp=drive_link\"\n",
        "#, \"12.2.7\": \"https://drive.google.com/file/d/1Ibmd32L3KNk-0QgdcVP1TCwrDvUoMsxe/view?usp=drive_link\"\n",
        "#, \"12.2.8\": \"https://drive.google.com/file/d/1YmjjdY0vclz5Exdm5rCpnasdPk4F6Elx/view?usp=drive_link\"\n",
        "#, \"12.3.1\": \"https://drive.google.com/file/d/1C9HdWjB3iE1FUsu_HS6DKs2s-IzJjhC1/view?usp=drive_link\"\n",
        "#, \"12.3.2\": \"https://drive.google.com/file/d/1dJiCnqrxIaYo65oWj6PnYfgiIib6cnWG/view?usp=drive_link\"\n",
        "#, \"12.3.3\": \"https://drive.google.com/file/d/1gOUDbvrhVB36PgaNm3t2oz1Jrb-uK2di/view?usp=drive_link\"\n",
        " \"12.3.4\": \"https://drive.google.com/file/d/18KEWG7U95hWhj465jS15L6FVopaOmeE8/view?usp=drive_link\"\n",
        "#, \"12.3.5\": \"https://drive.google.com/file/d/1NO-hRhFxdAon98jJz-7WSv2_p6SESKuh/view?usp=drive_link\"\n",
        "#, \"12.3.6\": \"https://drive.google.com/file/d/10152IOaFpTvZwEtkBXJUB6nw6QrHMm0z/view?usp=drive_link\"\n",
        "#, \"12.3.7\": \"https://drive.google.com/file/d/1_pLRwTEz0jFoxNoQCahrp3g8zb_y2CeP/view?usp=drive_link\"\n",
        "#, \"12.3.8\": \"https://drive.google.com/file/d/1Up36WPDmim6gGD88ffxjVCYtoRL_fjDw/view?usp=drive_link\"\n",
        "#, \"12.3.9\": \"https://drive.google.com/file/d/1JzWBv4R7CQ-R-iLDA8JIzvknZiHXcCfT/view?usp=drive_link\"\n",
        "#, \"12.3.10\": \"https://drive.google.com/file/d/1e1UzW4lFBsdKhy8JhjAtJuJNIMLG4A0V/view?usp=drive_link\"\n",
        "#, \"12.3.11\": \"https://drive.google.com/file/d/1OYwL3vkw_F04vJs4HaKzQDNmZ0wA_Mzj/view?usp=drive_link\"\n",
        "#, \"12.3.12\": \"https://drive.google.com/file/d/1i3gcXDKAF0RDQ0ysbMh-a4luyaF49SqA/view?usp=drive_link\"\n",
        "#, \"12.3.13\": \"https://drive.google.com/file/d/1DGlzPKr6I8CW_-95i6unhauoEykn_SH2/view?usp=drive_link\"\n",
        "#, \"12.3.14\": \"https://drive.google.com/file/d/1ELblG_HmiAP-8EOfSPQ796i49MmwTslf/view?usp=drive_link\"\n",
        "#, \"12.3.15\": \"https://drive.google.com/file/d/1rxYMfTClhDG2eJsOAX-gzjqzzfAhhydW/view?usp=drive_link\"\n",
        "}"
      ],
      "metadata": {
        "id": "nnbRyE8t-OLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Установка библиотек\n",
        "!pip install tiktoken langchain openai chromadb gspread oauth2client nltk pydantic==1.10.8 faiss-cpu python-docx"
      ],
      "metadata": {
        "id": "zIBD7aFKkfML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Импорт библиотек\n",
        "import gdown\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.docstore.document import Document\n",
        "import requests\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "import pathlib\n",
        "import subprocess\n",
        "import tempfile\n",
        "import ipywidgets as widgets\n",
        "import os\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import re\n",
        "import getpass\n",
        "import os\n",
        "import openai\n",
        "import tiktoken"
      ],
      "metadata": {
        "id": "s2dJmIzdel-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Получение ключа API от пользователя и установка его как переменной окружения\n",
        "openai_key = getpass.getpass(\"OpenAI API Key:\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "openai.api_key = openai_key"
      ],
      "metadata": {
        "id": "ms72WfoYemBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задача создания нейро-копирайтера\n",
        "## План создания нейро-копирайтера\n",
        "###• Исходные материалы: видео\n",
        "###• Распознавание речи через Whisper\n",
        "###• Разделение текста на чанки по 3000 токенов [у нас 2000]\n",
        "###• Саммаризация чанков до 1000 токенов [у нас 2000]\n",
        "###• Составление заголовков и подзаголовков по 5-10-15 чанкам [у нас 15]\n",
        "###• Улучшение списка заголовков\n",
        "###• Составление текста для каждого заголовка/подзаголовка\n",
        "###• Запуск нейро-куратора\n",
        "###• Тестирование нейро-куратора"
      ],
      "metadata": {
        "id": "44D-JX_u6-si"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Роль1 - Копирайтер - Системная роль\n",
        "###• system_prompt = \"\"\"\n",
        "###• Ты профессиональный копирайтер. У тебя большой опыт работы с Бизнесом в разных\n",
        "###• сферах, ты качественно структурируешь текст на Русском языке.\n",
        "###• Сделай формальный технический пересказ того, о чем рассказывает специалист в\n",
        "###• диалоге с другими клиентами. Пиши от имени Специалиста.\n",
        "###• Необходимо уложиться в 2000 токенов.\n",
        "###• \"\"\""
      ],
      "metadata": {
        "id": "OVM9dtTs7tcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Роль1 - Копирайтер - Инструкции в messages\n",
        "###• messages = [\n",
        "###• {\"role\": \"system\", \"content\": f\"{self.system_promt}\"},\n",
        "###• {\"role\": \"user\", \"content\": f\"\"\"\n",
        "###• Проанализируй отрывок материала с семинара: {materials}.\n",
        "###• дай компактный, сжатый, обобщенный список тем и подтем.\n",
        "###• Темы Необходимо оформить #, а подтемы оформить ##_.\n",
        "###• Используй только такой пример, ничего не добавляй лишнего.\n",
        "###• Пример составления списка:\n",
        "###• _#...\n",
        "###• ##_...\n",
        "###• _#...\n",
        "###• ##...\n",
        "###• \"\"\"}\n",
        "###• ]"
      ],
      "metadata": {
        "id": "d1MVXIQw8UGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Роль2 - Специалист - системная роль\n",
        "###• speaker_system_promt = \"\"\"\n",
        "###• Ты Специалист, готовящий текст для обучающего материала. Ты профессионал своего дела.\n",
        "###• Тебе предоставят отрывок Твоего Текста и попросят раскрыть подробнее одну из тем\n",
        "###• обучающего материала.\n",
        "###• Твоя цель: Опираясь только на Текст, подробно, развернуто рассказать по\n",
        "###• интересующей теме на русском языке.\n",
        "###• Необходимо уложиться в 6000 токенов.\n",
        "###• \"\"\"\n"
      ],
      "metadata": {
        "id": "hycHHI3k-Dkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Роль2 - Специалист - Инструкции в messages\n",
        "###• messages = [\n",
        "###• {\"role\": \"system\", \"content\": f\"{self.speaker_system_promt}\"),\n",
        "###• {\"role\": \"user\", \"content\": f\"\"\"\n",
        "###• Вот отрывки Вашего текста:\\n{message_content}\n",
        "###• Опираясь только на информацию из Вашего Текста, указанного выше, расскажите\n",
        "###• подробнее по теме: {topic}.\n",
        "###• \"\"\"}\n",
        "###• ]"
      ],
      "metadata": {
        "id": "hff7FOJQ-RWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "-wluTTlil9c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "import getpass\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ----------------------------------\n",
        "MODEL_TURBO_16K = \"gpt-3.5-turbo-16k\"\n",
        "MODEL_TURBO_0613 = \"gpt-3.5-turbo-0613\"\n",
        "MODEL_GPT4 = \"gpt-4-0613\"\n",
        "# ----------------------------------"
      ],
      "metadata": {
        "id": "fhKcHQUFmG8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Переводим текстовый файл в word\n",
        "import requests\n",
        "import docx\n",
        "\n",
        "def convert_txt_to_docx(url, output_path):\n",
        "    match_ = re.search('/file/d/([a-zA-Z0-9-_]+)', url)\n",
        "    if match_ is None:\n",
        "        raise ValueError('Invalid Google Docs URL')\n",
        "    doc_id = match_.group(1)\n",
        "\n",
        "    response = requests.get(\"https://docs.google.com/uc?export=download&id=\"+doc_id)\n",
        "    encoding = response.encoding\n",
        "    content = response.text\n",
        "\n",
        "    doc = docx.Document()\n",
        "    doc.add_paragraph(content.encode(encoding, 'ignore').decode('utf-8'))\n",
        "\n",
        "    doc.save(output_path)\n",
        "\n",
        "    print(encoding)\n",
        "\n",
        "def convert_txt_to_txt(url, output_path):\n",
        "    match_ = re.search('/file/d/([a-zA-Z0-9-_]+)', url)\n",
        "    if match_ is None:\n",
        "        raise ValueError('Invalid Google Docs URL')\n",
        "    doc_id = match_.group(1)\n",
        "\n",
        "    response = requests.get(\"https://docs.google.com/uc?export=download&id=\"+doc_id)\n",
        "    encoding = response.encoding\n",
        "    content = response.text\n",
        "\n",
        "    with open(output_path, \"w\") as fw:\n",
        "        fw.write(content.encode(encoding, 'ignore').decode('utf-8'))\n",
        "        fw.write(\"\\n\")\n",
        "\n",
        "    print(encoding)\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/KIA_VIDEO/Copywriter/KIA/BASE/\"\n",
        "for key in HOWTO_TEXTS:\n",
        "    convert_txt_to_txt(HOWTO_TEXTS[key], BASE + str(key) + \".txt\")\n"
      ],
      "metadata": {
        "id": "Sx3QYNeSuU-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class WorkerОpenAI():\n",
        "  def __init__(self, \\\n",
        "               system_promt = \" \", \\\n",
        "               system_promt_lector = \" \", \\\n",
        "               mod = MODEL_TURBO_16K, \\\n",
        "               content_topics = None, \\\n",
        "               save_project = '/content/'):\n",
        "    self.model = mod\n",
        "    self.save_project  = save_project\n",
        "\n",
        "    if content_topics:\n",
        "      self.content_topics = self.load_txt_file(content_topics)\n",
        "\n",
        "    # системные настройки\n",
        "    self.system_promt = self.load_document_text(system_promt)\n",
        "    self.speaker_system_promt = self.load_document_text(system_promt_lector)\n",
        "\n",
        "\n",
        "  def load_document_text(self, url: str) -> str:\n",
        "      # функция для загрузки документа по ссылке из гугл док\n",
        "      match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)\n",
        "      if match_ is None:\n",
        "          raise ValueError('Invalid Google Docs URL')\n",
        "      doc_id = match_.group(1)\n",
        "      response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')\n",
        "      response.raise_for_status()\n",
        "      text = response.text\n",
        "      return text\n",
        "\n",
        "\n",
        "  def load_txt_file(self, file_path):\n",
        "      with open(file_path, 'r') as file_:\n",
        "          text = file_.read()\n",
        "      return text\n",
        "\n",
        "  # пример подсчета токенов\n",
        "  def num_tokens_from_messages(self, messages):\n",
        "      \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
        "      try:\n",
        "          encoding = tiktoken.encoding_for_model(self.model)\n",
        "      except KeyError:\n",
        "          encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "      # if self.model == \"gpt-3.5-turbo-0301\":  # note: future models may deviate from this\n",
        "      if self.model in [\"gpt-3.5-turbo-0613\", \"gpt-3.5-turbo-16k\", \"gpt-4-0613\"]:  # note: future models may deviate from this\n",
        "          num_tokens = 0\n",
        "          for message in messages:\n",
        "              num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
        "              for key, value in message.items():\n",
        "                  num_tokens += len(encoding.encode(value))\n",
        "                  if key == \"name\":  # if there's a name, the role is omitted\n",
        "                      num_tokens += -1  # role is always required and always 1 token\n",
        "          num_tokens += 2  # every reply is primed with <im_start>assistant\n",
        "          return num_tokens\n",
        "      else:\n",
        "          raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {self.model}.\n",
        "  See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
        "\n",
        "\n",
        "  def f00_create_embedding_faiss_db(self, doc_txt_dir=\"/content/\", \\\n",
        "                                faiss_db_dir =\"/content/\", \\\n",
        "                                start_idx = 0, \\\n",
        "                                collection_name = \" \"):\n",
        "\n",
        "    def f01_num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "      \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "      encoding = tiktoken.get_encoding(encoding_name)\n",
        "      num_tokens = len(encoding.encode(string))\n",
        "      return num_tokens\n",
        "\n",
        "    # Для Копирайтера\n",
        "    self.splitter = RecursiveCharacterTextSplitter(['\\n\\n', '\\n', ' '], chunk_size=1024, chunk_overlap=300)\n",
        "    idx_file_folder = start_idx-1\n",
        "    chunkID = idx_file_folder\n",
        "\n",
        "    count_tokens = 0\n",
        "    # проходимся по всем данным\n",
        "    for _, file_ in enumerate(sorted(os.listdir(doc_txt_dir))):\n",
        "        print(\"Загружается файл: \", file_)\n",
        "        self.file_name = file_\n",
        "        idx_file_folder +=1\n",
        "        source_chunks = []\n",
        "        # разбиваем на несколько частей с помощью метода split_text\n",
        "        with open(doc_txt_dir + file_, \"r\") as f:\n",
        "          for chunk in self.splitter.split_text(f.read()):\n",
        "              chunkID += 1\n",
        "              source_chunks.append(Document(page_content=chunk, \\\n",
        "                                  metadata={'source': file_,\n",
        "                                            'chunkID': chunkID,\n",
        "                                            \"collection_name\": collection_name,\n",
        "                                            'idx_file_folder': idx_file_folder}))\n",
        "\n",
        "\n",
        "        # Создание индексов документа и СОХРАНЕНИЕ\n",
        "        # Если документ не пуст, то создать и сохранить базу индексов эмбеддингов отрезков документа\n",
        "        if len(source_chunks) > 0:\n",
        "            self.db = FAISS.from_documents(source_chunks, OpenAIEmbeddings())\n",
        "            count_token = f01_num_tokens_from_string(' '.join([x.page_content for x in source_chunks]), \"cl100k_base\")\n",
        "            count_tokens += count_token\n",
        "            print('Количество токенов в документе :', count_token)\n",
        "            # print('ЦЕНА запроса:', 0.0004 * (count_token / 1000), ' $')\n",
        "\n",
        "            self.db.save_local(os.path.join(faiss_db_dir, collection_name, f\"{str(idx_file_folder)}_db_initial__{file_[:20]}\"))\n",
        "\n",
        "    print('\\nЦЕНА запроса создания базы индексов:', 0.0004 * (count_tokens / 1000), ' $')\n",
        "\n",
        "  # ЗАПРС в ChatGPT\n",
        "  def get_ChatCompletion(self, model,  # указываем модель\n",
        "                         messages,     # словарь запроса\n",
        "                         temp=0.1):    # температуру\n",
        "\n",
        "      completion = openai.ChatCompletion.create(\n",
        "        model= model,\n",
        "        messages= messages,\n",
        "        temperature= temp\n",
        "        )\n",
        "\n",
        "      print(f'{completion[\"usage\"][\"total_tokens\"]} токенов использовано всего (вопрос-ответ).')\n",
        "      # print('ЦЕНА запроса с ответом :', 0.0015*(completion[\"usage\"][\"total_tokens\"]/1000), ' $')\n",
        "      print('===========================================: \\n')\n",
        "      return completion.choices[0].message.content\n",
        "\n",
        "  # ЗАПРС на создание темы и подтемы по Материалам\n",
        "  def get_search_materials_topics_subtopics(self, materials, model_topics = \"gpt-3.5-turbo-16k\"):\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": f\"{self.system_promt}\"},\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"Проанализируй отрывок материала из диалога с другими клиентами: {materials}.\n",
        "• дай компактный, сжатый, обобщенный список тем и подтем.\n",
        "• Темы Необходимо оформить #, а подтемы оформить ##_.\n",
        "• Используй только такой пример, ничего не добавляй лишнего.\n",
        "• Пример составления списка:\n",
        "Пример составления списка:\n",
        "_#...\n",
        "##_...\n",
        "_#...\n",
        "##_...\n",
        "        \"\"\"}\n",
        "        ]\n",
        "\n",
        "    # example token count from the function defined above\n",
        "    print(f\"{self.num_tokens_from_messages(messages=messages)} токенов использовано на вопрос \\n\")\n",
        "    try:\n",
        "      self.content_topics += self.get_ChatCompletion(model_topics, messages)\n",
        "    except:\n",
        "      print(\"Модель в настоящее время перегружена. Попробуйте позже.\")\n",
        "\n",
        "\n",
        "  # ПОИСК ТЕМЫ и ПОДТЕМЫ\n",
        "  def search_topics_subtopics(self, num_chunk = 10):\n",
        "    self.content_topics = ''\n",
        "    materials = \"\"\n",
        "    # Выбираем блоки ПОДРЯД\n",
        "    len_chunk = len(self.db.docstore._dict)\n",
        "    if len_chunk < num_chunk + 1:\n",
        "        for _, doc in self.db.docstore._dict.items():\n",
        "            materials += f\"{doc.page_content}\\n\"\n",
        "        self.get_search_materials_topics_subtopics(materials)\n",
        "    else:\n",
        "        for ind, (key, doc) in enumerate(self.db.docstore._dict.items()):\n",
        "          materials += f\"{doc.page_content}\\n\"\n",
        "          if (ind+1) % num_chunk == 0:\n",
        "            self.get_search_materials_topics_subtopics(materials)\n",
        "            materials = \"\"\n",
        "    if materials != \"\":\n",
        "        self.get_search_materials_topics_subtopics(materials)\n",
        "\n",
        "    print('Собрали список тем и подтем: ')\n",
        "    print(self.content_topics)\n",
        "\n",
        "    with open(f'{self.save_project}{self.file_name[:20]}__темы_подтемы.txt', \"w\") as f:\n",
        "      f.write(self.content_topics)\n",
        "\n",
        "\n",
        "  # ОБЪЕДИНЯЕМ схожие ТЕМЫ\n",
        "  def get_merge_topics(self, model_topics = \"gpt-3.5-turbo-16k\"):\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": f\"{self.system_promt}\"},\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"Проанализируй Темы и подтемы из Текста диалогов специалиста с другими клиентами: {self.content_topics}.\n",
        "\n",
        "Необходимо объединить похожие по смыслу темы или подтемы, записать компактно.\n",
        "При необходимости перефразировать тему или подтему. Дай корректный список.\n",
        "Темы оформи _#, а подтемы оформи ##_.\n",
        "Используй только такой пример, ничего не добавляй лишнего.\n",
        "Пример составления списка:\n",
        "_#...\n",
        "##_...\n",
        "_#...\n",
        "##_...\n",
        "\"\"\"}\n",
        "]\n",
        "    # example token count from the function defined above\n",
        "    print(f\"{self.num_tokens_from_messages(messages=messages)} токенов использовано на вопрос \\n\")\n",
        "    try:\n",
        "      self.content_topics = self.get_ChatCompletion(model_topics, messages)\n",
        "      print('Итоговый список тем и подтем: ')\n",
        "      print(self.content_topics)\n",
        "      with open(f'{self.save_project}{self.file_name[:20]}__темы_подтемы_ИТОГ.txt', \"w\") as f:\n",
        "        f.write(self.content_topics)\n",
        "    except:\n",
        "      print(\"Модель в настоящее время перегружена. Попробуйте позже.\")\n",
        "\n",
        "  # ОРГАНИЗУЕМ текст блоками\n",
        "  def organize_text(self, topic, num_chunks, model_topics):\n",
        "\n",
        "    # Выборка документов по схожести с подтемой\n",
        "    docs = self.db.similarity_search(topic, k = num_chunks)\n",
        "    message_content = '\\n'.join([doc.page_content + '\\n' for i, doc in enumerate(docs)])\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": f\"{self.speaker_system_promt}\"},\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"Вот отрывки Вашей лекции:\\n{message_content}\n",
        "\n",
        "Опираясь только на информацию с Лекции, указанной выше, расскажите подробнее по Теме: {topic}.\n",
        "\"\"\"}\n",
        "      ]\n",
        "\n",
        "    # example token count from the function defined above\n",
        "    print('\\n ===========================================: ')\n",
        "    print(f\"{self.num_tokens_from_messages(messages=messages)} токенов использовано на вопрос \\n\")\n",
        "    try:\n",
        "      self.final_text += self.get_ChatCompletion(model_topics, messages)\n",
        "    except:\n",
        "      print(\"Модель в настоящее время перегружена. Попробуйте позже.\")\n",
        "\n",
        "  # ОРГАНИЗУЕМ ИТОГОВЫЙ ТЕКСТ\n",
        "  def organize_final_text(self, num_chunks = 10,\n",
        "                          model_topics = \"gpt-3.5-turbo-16k\",\n",
        "                          name = None,\n",
        "                          db_path = None):\n",
        "    if name:\n",
        "      self.file_name = name\n",
        "\n",
        "    if db_path:\n",
        "      for curr_base in os.listdir(db_path):\n",
        "          self.db = FAISS.load_local(os.path.join(db_path, curr_base), OpenAIEmbeddings())\n",
        "\n",
        "    self.final_text = \"\"\n",
        "    list_topics = self.content_topics.split('\\n')\n",
        "    # проходимся по списку тем и подтем\n",
        "    for ind, topic in tqdm(enumerate(list_topics)):\n",
        "        # тему просто записываем в итоговый текст\n",
        "        if (\"_#\" in topic) and not(\"_#\" in list_topics[ind+1]):\n",
        "          self.final_text += f'\\n{topic}\\n'\n",
        "        # подтему передаем в ChatGPT\n",
        "        else:\n",
        "          self.final_text += f'\\n{topic}\\n'\n",
        "          self.organize_text(topic, num_chunks, model_topics)\n",
        "\n",
        "    with open(f'{self.save_project}{self.file_name[:20]}__final_text.txt', \"w\") as f:\n",
        "      f.write(self.final_text)\n",
        "    print(\"\\nСоздали финальный документ для Базы Знаний (final_text)\")\n"
      ],
      "metadata": {
        "id": "XIOx0m03mSiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Copywriter"
      ],
      "metadata": {
        "id": "LEB6lfh3vRYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Создаем объект для работы Copywriter\n",
        "projects_dir = '/content/drive/MyDrive/KIA_VIDEO/Copywriter/KIA/'\n",
        "\n",
        "Promt_copywriter = \"https://docs.google.com/document/d/1D9lpYTUfLLr-25pnvbHUXK9mXv-rPRiFW3s92UMnPoE/edit?usp=sharing\"\n",
        "Promt_lector = \"https://docs.google.com/document/d/1Lf70xX7_brwNE7FItvQbNtf51yR47Fuam7SorkvUe8s/edit?usp=sharing\"\n",
        "\n",
        "# если темы созданы\n",
        "# topics_final = \"/content/drive/MyDrive/Colab Notebooks/_Projects_ChatGPT/Нейро_Copywriter/УИИ/___.txt\"\n",
        "# # Создаем объект для дообучения chatGPT\n",
        "curator = WorkerОpenAI(system_promt = Promt_copywriter, # системный промт\n",
        "                       system_promt_lector = Promt_lector, # промт Лектора\n",
        "                      #  content_topics = topics_final,\n",
        "                       save_project = projects_dir)     # путь для сохранения готовых файлов\n",
        "\n",
        "# Формируем базу по файлу txt\n",
        "# путь к материалам\n",
        "doc_txt_dir = projects_dir + 'BASE/'\n",
        "db_initial = projects_dir\n",
        "\n",
        "curator.f00_create_embedding_faiss_db(doc_txt_dir = doc_txt_dir,   # путь к материалам\n",
        "                                faiss_db_dir = db_initial,     # путь для сохранения исходной базы\n",
        "                                start_idx = 0,                 # номер документа в базе\n",
        "                                collection_name = 'KIA_db_initial')  # наименование коллекции"
      ],
      "metadata": {
        "id": "KYNb1d6q4hWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Смотрим созданные чанки\n",
        "print(f\"Текст разбит на чанки. Всего: {len(curator.db.docstore._dict.values())} шт.\\n\")\n",
        "curator.db.docstore._dict.values()"
      ],
      "metadata": {
        "id": "sGRXeHMLCfpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Поиск темы и подтемы выборкой по 5 - 15 чанков\n",
        "\n",
        "# Подаем в цикле и просим составить список тем и подтем.\n",
        "curator.search_topics_subtopics(num_chunk = 15)"
      ],
      "metadata": {
        "id": "3TLVJMHbJXPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Корректируем список тем и подтем (обобщаем, убираем дубли).\n",
        "# Подаем сформированный Список тем и просим объединить похожие по смыслу темы или подтемы, записать компактно.\n",
        "# При необходимости перефразировать тему или подтему.\n",
        "curator.get_merge_topics()"
      ],
      "metadata": {
        "id": "xbrAFSweUxC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ОРГАНИЗУЕМ ИТОГОВЫЙ ТЕКСТ"
      ],
      "metadata": {
        "id": "qIm6znHAtWVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выборка блоков документов по схожести с подтемой. Передаем собранные отрывки с Лекции и просим рассказать подробнее, опираясь на Лекцию."
      ],
      "metadata": {
        "id": "2fWybucTLHnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# запускаем сбору Итогового Текста\n",
        "# name_file = 'Разбор нейро-сотрудников на chatGPT _ 21.08.2023'\n",
        "# db_initial = '/content/drive/MyDrive/Colab Notebooks/_Projects_ChatGPT/Нейро_Copywriter/УИИ/УИИ_db_initial'\n",
        "\n",
        "# curator.organize_final_text(name = name_file, db_path = db_initial)\n",
        "curator.organize_final_text()"
      ],
      "metadata": {
        "id": "tzRnco4xRX19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# сформирован итоговый текст\n",
        "curator.final_text"
      ],
      "metadata": {
        "id": "iETHjZJRTnFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MarkdownHeaderTextSplitter"
      ],
      "metadata": {
        "id": "uB6aUKeui5Hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title функции\n",
        "chat_manager_system = \"\"\"\n",
        "Ты Специалист, готовящий текст для обучающего материала. Ты профессионал своего дела.\n",
        "Тебе предоставят отрывок Твоего Текста и попросят раскрыть подробнее одну из тем обучающего материала.\n",
        "Твоя цель: Опираясь только на Текст, подробно, развернуто рассказать по интересующей теме на русском языке.\n",
        "Необходимо уложиться в 6000 токенов.\n",
        "\"\"\"\n",
        "\n",
        "def get_chatgpt_answer(topic,  db, model = MODEL_TURBO_0613):\n",
        "  # Выборка документов по схожести с вопросом\n",
        "  docs = db.similarity_search(topic, k=4)\n",
        "  message_content = re.sub(r'\\n{2}', ' ', '\\n '.join([f'\\n  ' + doc.page_content + '\\n' for i, doc in enumerate(docs)]))\n",
        "  # print('message_content :\\n ======================================== \\n', self.message_content)\n",
        "\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": f\"{chat_manager_system}\"},\n",
        "    {\"role\": \"user\", \"content\": f\"\"\"Analyze step by step and give a detailed correct answer to the Student's question.\\n\n",
        "    Question:\\n{topic}\\n\\nMaterials from the Webinar:\\n{message_content}\\n\\nAnswer:\"\"\"}\n",
        "    ]\n",
        "\n",
        "  try:\n",
        "    completion = openai.ChatCompletion.create(\n",
        "    model=model,\n",
        "    messages=messages,\n",
        "    temperature=0.1\n",
        "    )\n",
        "\n",
        "    print(f'{completion[\"usage\"][\"total_tokens\"]} токенов использовано всего (вопрос-ответ).')\n",
        "    print('ЦЕНА запроса с ответом :', 0.0015*(completion[\"usage\"][\"total_tokens\"]/1000), ' $')\n",
        "    print('===========================================: \\n')\n",
        "    print('Ответ ChatGPT: ')\n",
        "    print(completion.choices[0].message.content)\n",
        "    # return completion.choices[0].message.content\n",
        "  except:\n",
        "    print(\"Модель в настоящее время перегружена. Попробуйте позже.\")\n",
        "\n",
        "def load_txt_file(file_path):\n",
        "    with open(file_path, 'r') as file_:\n",
        "        text = file_.read()\n",
        "    return text\n",
        "\n",
        "projects_dir = '/content/drive/MyDrive/KIA_VIDEO/Copywriter/KIA/'\n",
        "name_file = '12.3.4.txt__final_text.txt' # FIXME\n",
        "final_text_dir = projects_dir + name_file\n",
        "\n",
        "# Загружаем итоговый текст\n",
        "final_text = load_txt_file(final_text_dir)\n"
      ],
      "metadata": {
        "id": "IN3nV98Ti14W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  Готовим документ MarkdownHeader по сформированному тексту\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "\n",
        "headers_to_split_on = [\n",
        "    (\"_#\", \"Header 1\"),\n",
        "    (\"##_\", \"Header 2\"),\n",
        "    (\"###_\", \"Header 3\"),\n",
        "    (\"####_\", \"Header 4\"),\n",
        "    (\"#####_\", \"Header 5\"),\n",
        "]\n",
        "\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "md_header_splits = markdown_splitter.split_text(final_text)\n",
        "md_header_splits"
      ],
      "metadata": {
        "id": "yno60mdeR9lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1024, chunk_overlap=300\n",
        ")\n",
        "\n",
        "# Split\n",
        "split = text_splitter.split_documents(md_header_splits)\n",
        "db = FAISS.from_documents(split, OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "I4IBWVoNu63m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split[1]"
      ],
      "metadata": {
        "id": "_yu7ynmx63e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic1 = \"\"\"\n",
        "При установке детского кресла по центру сидения пассажиры третьего ряда могут сидеть лицом к ребенку, не так ли?\n",
        "\"\"\"\n",
        "\n",
        "get_chatgpt_answer(topic1,  db)"
      ],
      "metadata": {
        "id": "nxRm8e134sjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic2 = \"\"\"\n",
        "Как снять сидения второго ряда?\n",
        "\"\"\"\n",
        "\n",
        "get_chatgpt_answer(topic2,  db)"
      ],
      "metadata": {
        "id": "QFGtb7-5tc6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic3 = \"\"\"\n",
        "Какую длину и ширину имеет погрузочное пространство?\n",
        "\"\"\"\n",
        "\n",
        "get_chatgpt_answer(topic3,  db)"
      ],
      "metadata": {
        "id": "SvHg-dkxtiTK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}