{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogALH2Bj4bhX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import openpyxl\n",
        "import re\n",
        "import requests\n",
        "import threading, time\n",
        "import json\n",
        "import os\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def savetxt(dir, name, text):\n",
        "  if not os.path.exists('result'): os.mkdir('result')\n",
        "  if not os.path.exists('result/'+dir): os.mkdir('result/'+dir)\n",
        "  if os.path.exists('result/'+dir+'/'+name):\n",
        "    print('Файл существует')\n",
        "    return None\n",
        "  with open('result/'+dir+'/'+name,'w',encoding='utf-8') as f:\n",
        "    f.write(text)\n",
        "  return True\n",
        "\n",
        "def resulttotext(r):\n",
        "  if r is not None:\n",
        "    return r.text+'.'\n",
        "  else:\n",
        "    return ''\n",
        "\n",
        "def parser(soup):\n",
        "    text = ''\n",
        "    pages = soup.findAll('div', class_='g-padding')\n",
        "    if pages is None:\n",
        "      print(soup.find('title').text.split(' – ')[0])\n",
        "      print(soup)\n",
        "    else:\n",
        "      for page in pages:\n",
        "        if page.find('div',class_='faq') is not None:\n",
        "          text += page.find('div',class_='faq').text\n",
        "        else:\n",
        "          content = page.select('div[class*=\"text-\"]')\n",
        "          text += ''.join(i.text+'\\n' for i in content)\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "QB8wUoSXBnHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pars page whith models\n",
        "\n",
        "pagemodels = requests.get('https://www.kia.ru/models/').text\n",
        "modellinks = []\n",
        "\n",
        "html = ''.join(line.strip() for line in pagemodels.split(\"\\n\"))\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "cards_list = soup.find_all('div', class_='car-card')\n",
        "\n",
        "for card in cards_list:\n",
        "  modellinks.append('https://www.kia.ru'+ str(card.a['href']))\n"
      ],
      "metadata": {
        "id": "2UBgjdUqy2l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getmodelsoup(url):\n",
        "  model = requests.get(url).text\n",
        "  html = ''.join(line.strip() for line in model.split(\"\\n\"))\n",
        "  return BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "modeldict = {}\n",
        "\n",
        "for i in modellinks:\n",
        "  modeldict[i] = getmodelsoup(i)\n"
      ],
      "metadata": {
        "id": "bYOQsbrt2Tvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stringstolist(div):\n",
        "  '''Превращаем блок в список строк'''\n",
        "  lis = []\n",
        "  try:\n",
        "    for string in div.strings:\n",
        "      if string not in [' ','']:\n",
        "        lis.append(string.text)\n",
        "  except:\n",
        "    lis = ['']\n",
        "  return lis\n",
        "\n",
        "\n",
        "def parsermodel(soup):\n",
        "  text = ''\n",
        "\n",
        "  # Обрабатываю по id basic\n",
        "  basic = soup.select('div[id*=\"basic_\"]')\n",
        "  for div in basic:\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)\n",
        "    try:\n",
        "      img = div.find('img')['data-src']\n",
        "    except:\n",
        "      img = ''\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:])}\\n[IMG {img}]\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id dizain\n",
        "  dizain = soup.select('div[id*=\"dizain_\"]')\n",
        "  for div in dizain:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id eksterer\n",
        "  eksterer = soup.select('div[id*=\"eksterer_\"]')\n",
        "  for div in eksterer:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id interer\n",
        "  interer = soup.select('div[id*=\"interer_\"]')\n",
        "  for div in interer:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id style\n",
        "  style = soup.select('div[id*=\"style_\"]')\n",
        "  for div in style:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id multimedia\n",
        "  multimedia = soup.select('div[id*=\"multimedia_\"]')\n",
        "  for div in multimedia:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id tehnologii\n",
        "  tehnologii = soup.select('div[id*=\"tehnologii_\"]')\n",
        "  for div in tehnologii:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id bezopasnost\n",
        "  bezopasnost = soup.select('div[id*=\"bezopasnost_\"]')\n",
        "  for div in bezopasnost:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id teplye_opcii\n",
        "  teplye_opcii = soup.select('div[id*=\"teplye_opcii_\"]')\n",
        "  for div in teplye_opcii:\n",
        "    if len(div['id'].split('_')) > 3: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id komfort\n",
        "  komfort = soup.select('div[id*=\"komfort_\"]')\n",
        "  for div in komfort:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id vmestimost\n",
        "  vmestimost = soup.select('div[id*=\"vmestimost_\"]')\n",
        "  for div in vmestimost:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id dvigatel\n",
        "  dvigatel = soup.select('div[id*=\"dvigatel_\"]')\n",
        "  for div in dvigatel:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    text += 'Модели двигателей: '\n",
        "    ul = div.find('ul')\n",
        "    for t in stringstolist(ul):\n",
        "      text += t+', '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  return text\n",
        "\n",
        "def parseroptions(url):\n",
        "  text = '## Комплектации\\n'\n",
        "  links = []\n",
        "  url = url.replace('desc','options')\n",
        "  suop = getmodelsoup(url)\n",
        "  ahrefs = suop.findAll('div',class_='config__variants__slide')\n",
        "  for a in ahrefs:\n",
        "    cont = a.find('li').text\n",
        "    url = a.find('a')\n",
        "    option = getmodelsoup('https://www.kia.ru'+url['href'])\n",
        "    titel = option.find('title').text\n",
        "    text += f'### Комплектация: {titel}\\nЦена: {cont}\\n'\n",
        "    info = option.findAll('div', class_=\"info-section\")\n",
        "\n",
        "    for i in info:\n",
        "      t2 = i.find('div', class_=\"info-section__header\").text\n",
        "      text += t2 + ': '\n",
        "      if t2.strip() == 'Технические характеристики' or t2.strip() == 'Спецификация':\n",
        "        dl = i.findAll('dl')\n",
        "        for j in dl:\n",
        "          text += j.find('dt').text + ': ' + j.find('dd').text + '; '\n",
        "      else:\n",
        "        li = i.findAll('li')\n",
        "        for j in li:\n",
        "          text += j.text + ', '\n",
        "      text = text[:-2] + '\\n'\n",
        "\n",
        "  return text\n",
        "\n",
        "def resultsmodel(link):\n",
        "\n",
        "  print(link)\n",
        "  name = modeldict[link].find('title').text.split(' – ')[0]\n",
        "  name = name.replace('/','-')\n",
        "  text = f'#  {name} - [link {link}]\\n'\n",
        "  text += parsermodel(modeldict[link])\n",
        "  text += parseroptions(link)\n",
        "  savetxt('models', name+'.txt', text)\n",
        "  print('Done '+ name)\n",
        "\n",
        "\n",
        "threads = []\n",
        "# Добавляю потоки с функцией сохранения в файл в список потоков\n",
        "for link in modellinks:\n",
        "  threads.append(threading.Thread(target=resultsmodel, args=(link,)))\n",
        "\n"
      ],
      "metadata": {
        "id": "r-FezNzNhgEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Технологии\n",
        "\n",
        "def parserabout(soup):\n",
        "    text = '## '\n",
        "    pages = soup.find('div', class_='articles-detail__technology-txt')\n",
        "    if pages is None:\n",
        "      print(soup.find('title').text.split(' – ')[0])\n",
        "      print(soup)\n",
        "    else:\n",
        "        content = pages.select('div[class*=\"text-\"]')\n",
        "        text += ''.join(i.text+'\\n' for i in content)\n",
        "    return text\n",
        "\n",
        "url = \"https://www.kia.ru/ajax/page/technologies/more?limit=45&page=1\"\n",
        "\n",
        "headers = {\n",
        "    \"Referer\": \"https://www.kia.ru/about/technologies/\",  # Example Referer header\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "json_data = response.text\n",
        "\n",
        "data = json.loads(json_data)\n",
        "\n",
        "ids = [tech['id'] for tech in data['content']['technologies']]\n",
        "\n",
        "static_url = 'https://www.kia.ru/about/technologies/'\n",
        "urls = []\n",
        "\n",
        "for id in ids:\n",
        "    urls.append(static_url+id)\n",
        "\n",
        "def resultsabout(link):\n",
        "\n",
        "  print(link)\n",
        "  content = requests.get(link)\n",
        "  if content.status_code == 200:\n",
        "    #constructor-block\n",
        "    text = ''\n",
        "    html = ''.join(line.strip() for line in content.text.split(\"\\n\"))\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    name = soup.find('title').text.split(' – ')[0]\n",
        "    text += parserabout(soup)\n",
        "  savetxt('about', name.replace('/','-')+'.txt', text)\n",
        "  print('Done '+ name)\n",
        "print(urls)\n",
        "for link in urls:\n",
        "\n",
        "# Добавляю потоки с функцией сохранения в файл в список потоков\n",
        "  threads.append(threading.Thread(target=resultsabout, args=(link,)))"
      ],
      "metadata": {
        "id": "uvVI0zBxnwCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Start all threads\n",
        "for x in threads:\n",
        "  x.start()\n",
        "  time.sleep(0.5)\n",
        "\n",
        " # Wait for all of them to finish\n",
        "for x in threads:\n",
        "  x.join()"
      ],
      "metadata": {
        "id": "rj6SNz9s5eo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"# Технологии\" > result.txt\n",
        "!cat result/about/*.txt >> result.txt\n",
        "!cat result/models/*.txt >> result.txt\n",
        "!zip -r result.zip result result.txt"
      ],
      "metadata": {
        "id": "-oFV2vfa4ZNs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}