{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raTrjmB59OtT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import openpyxl\n",
        "import re\n",
        "import requests\n",
        "import threading, time\n",
        "import json\n",
        "import os\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def savetxt(dir, name, text):\n",
        "  if not os.path.exists('result'): os.mkdir('result')\n",
        "  if not os.path.exists('result/'+dir): os.mkdir('result/'+dir)\n",
        "  with open('result/'+dir+'/'+name,'w',encoding='utf-8') as f:\n",
        "    f.write(text)\n",
        "  return True\n",
        "\n",
        "def resulttotext(r):\n",
        "  if r is not None:\n",
        "    return r.text+'.'\n",
        "  else:\n",
        "    return ''\n",
        "\n",
        "def parser(soup):\n",
        "    text = ''\n",
        "    pages = soup.findAll('div', class_='g-padding')\n",
        "    if pages is None:\n",
        "      print(soup.find('title').text.split(' – ')[0])\n",
        "      print(soup)\n",
        "    else:\n",
        "      for page in pages:\n",
        "        if page.find('div',class_='faq') is not None:\n",
        "          text += page.find('div',class_='faq').text\n",
        "        else:\n",
        "          content = page.select('div[class*=\"text-\"]')\n",
        "          text += ''.join(i.text+'\\n' for i in content)\n",
        "    return text\n",
        "\n",
        "# Pars page whith models\n",
        "\n",
        "pagemodels = requests.get('https://www.kia.ru/models/').text\n",
        "modellinks = []\n",
        "\n",
        "html = ''.join(line.strip() for line in pagemodels.split(\"\\n\"))\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "cards_list = soup.find_all('div', class_='car-card')\n",
        "\n",
        "for card in cards_list:\n",
        "  modellinks.append('https://www.kia.ru'+ str(card.a['href']))\n",
        "\n",
        "def getmodelsoup(url):\n",
        "  model = requests.get(url).text\n",
        "  html = ''.join(line.strip() for line in model.split(\"\\n\"))\n",
        "  return BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "modeldict = {}\n",
        "\n",
        "for i in modellinks:\n",
        "  modeldict[i] = getmodelsoup(i)\n",
        "\n",
        "def stringstolist(div):\n",
        "  '''Превращаем блок в список строк'''\n",
        "  lis = []\n",
        "  try:\n",
        "    for string in div.strings:\n",
        "      if string not in [' ','']:\n",
        "        lis.append(string.text)\n",
        "  except:\n",
        "    lis = ['']\n",
        "  return lis\n",
        "\n",
        "\n",
        "def parsermodel(soup):\n",
        "  text = ''\n",
        "\n",
        "  # Обрабатываю по id basic\n",
        "  basic = soup.select('div[id*=\"basic_\"]')\n",
        "  for div in basic:\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)\n",
        "    try:\n",
        "      img = div.find('img')['data-src']\n",
        "    except:\n",
        "      img = ''\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:])}\\nФото {img}\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id dizain\n",
        "  dizain = soup.select('div[id*=\"dizain_\"]')\n",
        "  for div in dizain:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id eksterer\n",
        "  eksterer = soup.select('div[id*=\"eksterer_\"]')\n",
        "  for div in eksterer:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id interer\n",
        "  interer = soup.select('div[id*=\"interer_\"]')\n",
        "  for div in interer:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id style\n",
        "  style = soup.select('div[id*=\"style_\"]')\n",
        "  for div in style:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id multimedia\n",
        "  multimedia = soup.select('div[id*=\"multimedia_\"]')\n",
        "  for div in multimedia:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id tehnologii\n",
        "  tehnologii = soup.select('div[id*=\"tehnologii_\"]')\n",
        "  for div in tehnologii:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id bezopasnost\n",
        "  bezopasnost = soup.select('div[id*=\"bezopasnost_\"]')\n",
        "  for div in bezopasnost:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id teplye_opcii\n",
        "  teplye_opcii = soup.select('div[id*=\"teplye_opcii_\"]')\n",
        "  for div in teplye_opcii:\n",
        "    if len(div['id'].split('_')) > 3: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id komfort\n",
        "  komfort = soup.select('div[id*=\"komfort_\"]')\n",
        "  for div in komfort:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id vmestimost\n",
        "  vmestimost = soup.select('div[id*=\"vmestimost_\"]')\n",
        "  for div in vmestimost:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id dvigatel\n",
        "  dvigatel = soup.select('div[id*=\"dvigatel_\"]')\n",
        "  for div in dvigatel:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    text += 'Модели двигателей: '\n",
        "    ul = div.find('ul')\n",
        "    for t in stringstolist(ul):\n",
        "      text += t+', '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  return text\n",
        "\n",
        "def parseroptions(url):\n",
        "  text = '## Комплектации\\n'\n",
        "  links = []\n",
        "  url = url.replace('desc','options')\n",
        "  suop = getmodelsoup(url)\n",
        "  ahrefs = suop.findAll('div',class_='config__variants__slide')\n",
        "  for a in ahrefs:\n",
        "    cont = a.find('li').text\n",
        "    url = a.find('a')\n",
        "    option = getmodelsoup('https://www.kia.ru'+url['href'])\n",
        "    titel = option.find('title').text\n",
        "    text += f'### Комплектация: {titel}\\nЦена: {cont}\\n'\n",
        "    info = option.findAll('div', class_=\"info-section\")\n",
        "\n",
        "    for i in info:\n",
        "      t2 = i.find('div', class_=\"info-section__header\").text\n",
        "      text += t2 + ': '\n",
        "      if t2.strip() == 'Технические характеристики' or t2.strip() == 'Спецификация':\n",
        "        dl = i.findAll('dl')\n",
        "        for j in dl:\n",
        "          text += j.find('dt').text + ': ' + j.find('dd').text + '; '\n",
        "      else:\n",
        "        li = i.findAll('li')\n",
        "        for j in li:\n",
        "          text += j.text + ', '\n",
        "      text = text[:-2] + '\\n'\n",
        "\n",
        "  return text\n",
        "\n",
        "def resultsmodel(link):\n",
        "\n",
        "  print(link)\n",
        "  name = modeldict[link].find('title').text.split(' – ')[0]\n",
        "  name = name.replace('/','-')\n",
        "  text = f'#  {name} - [link {link}]\\n'\n",
        "  text += parsermodel(modeldict[link])\n",
        "  text += parseroptions(link)\n",
        "  savetxt('models', name+'.txt', text)\n",
        "  print('Done '+ name)\n",
        "\n",
        "\n",
        "threads = []\n",
        "# Добавляю потоки с функцией сохранения в файл в список потоков\n",
        "for link in modellinks:\n",
        "  threads.append(threading.Thread(target=resultsmodel, args=(link,)))\n",
        "\n",
        "# Технологии\n",
        "\n",
        "def parserabout(soup):\n",
        "    text = '## '\n",
        "    pages = soup.find('div', class_='articles-detail__technology-txt')\n",
        "    if pages is None:\n",
        "      print(soup.find('title').text.split(' – ')[0])\n",
        "      print(soup)\n",
        "    else:\n",
        "        content = pages.select('div[class*=\"text-\"]')\n",
        "        text += ''.join(i.text+'\\n' for i in content)\n",
        "    return text\n",
        "\n",
        "url = \"https://www.kia.ru/ajax/page/technologies/more?limit=45&page=1\"\n",
        "\n",
        "headers = {\n",
        "    \"Referer\": \"https://www.kia.ru/about/technologies/\",  # Example Referer header\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "json_data = response.text\n",
        "\n",
        "data = json.loads(json_data)\n",
        "\n",
        "ids = [tech['id'] for tech in data['content']['technologies']]\n",
        "\n",
        "static_url = 'https://www.kia.ru/about/technologies/'\n",
        "urls = []\n",
        "\n",
        "for id in ids:\n",
        "    urls.append(static_url+id)\n",
        "\n",
        "def resultsabout(link):\n",
        "\n",
        "  print(link)\n",
        "  content = requests.get(link)\n",
        "  if content.status_code == 200:\n",
        "    #constructor-block\n",
        "    text = ''\n",
        "    html = ''.join(line.strip() for line in content.text.split(\"\\n\"))\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    name = soup.find('title').text.split(' – ')[0]\n",
        "    text += parserabout(soup)\n",
        "  savetxt('about', name.replace('/','-')+'.txt', text)\n",
        "  print('Done '+ name)\n",
        "print(urls)\n",
        "for link in urls:\n",
        "\n",
        "# Добавляю потоки с функцией сохранения в файл в список потоков\n",
        "  threads.append(threading.Thread(target=resultsabout, args=(link,)))\n",
        "\n",
        "# Сбор с закладки \"Журнал\"\n",
        "\n",
        "url = \"https://www.kia.ru/ajax/page/mediacenter/magazine/more?limit=100&page=1\"\n",
        "static_url = \"https://www.kia.ru/press/magazine/\"\n",
        "HEADERS = {\"Referer\": static_url}\n",
        "\n",
        "response = requests.get(url=url, headers=HEADERS)\n",
        "json_data = response.text\n",
        "data = json.loads(json_data)\n",
        "all_article_list = []\n",
        "\n",
        "for article in data[\"content\"][\"media_center\"][\"magazine\"]:\n",
        "    code = article[\"code\"]\n",
        "    all_article_list.append(static_url + code + '/')\n",
        "\n",
        "url = \"https://www.kia.ru/ajax/page/mediacenter/news/more?limit=100&page=1\"\n",
        "static_url = \"https://www.kia.ru/press/news/\"\n",
        "response = requests.get(url=url, headers=HEADERS)\n",
        "json_data = response.text\n",
        "data = json.loads(json_data)\n",
        "all_article_list = []\n",
        "\n",
        "for article in data[\"content\"][\"media_center\"][\"news\"]:\n",
        "    code = article[\"code\"]\n",
        "    all_article_list.append(static_url + code + '/')\n",
        "\n",
        "def resultspress(link):\n",
        "\n",
        "  print(link)\n",
        "  content = requests.get(link)\n",
        "  if content.status_code == 200:\n",
        "    #constructor-block\n",
        "    text = ''\n",
        "    html = ''.join(line.strip() for line in content.text.split(\"\\n\"))\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    name = soup.find('title').text.split(' – ')[0]\n",
        "    head_press = soup.h1.text.strip() #заголовок статьи\n",
        "    all_img_list, all_img = [], \"\"\n",
        "    try:\n",
        "      [all_img_list.append(img.find(\"img\").attrs.get(\"src\")) for img in soup.find_all(\"div\", class_=\"articles-detail__content__offset\")]  # все фото из статьи\n",
        "      for img in all_img_list:\n",
        "        all_img += img + \",\"\n",
        "    except:\n",
        "      all_img = \"\"\n",
        "\n",
        "    try:\n",
        "      date_press = soup.select_one(\"div.articles-detail__date\").text.strip()  # дата статьи\n",
        "      for val in soup.find_all(\"div\", class_=\"g-container\"):\n",
        "        for child in val.children:\n",
        "          if date_press in child.text:\n",
        "            text += f\"## {head_press}>\\n\"\n",
        "            text += child.text.replace(\"\\xa0\", \" \")\n",
        "            text += f\"\\nФотографии из статьи: {all_img[:-1]}\\n\\n\"\n",
        "    except:\n",
        "      for val in soup.find_all(\"div\", class_=\"g-container\"):\n",
        "        for child in val.children:\n",
        "          if child.find(\"h1\") is not None and child.find(\"h1\") != -1:\n",
        "            text += f\"## {head_press}\\n\"\n",
        "            text += child.text.replace(\"\\xa0\", \" \")\n",
        "            text += f\"\\nФотографии из статьи: {all_img[:-1]}\\n\\n\"\n",
        "    if text != '':\n",
        "      savetxt('press', name.replace('/','-')+'.txt', text)\n",
        "      print('Done '+ name)\n",
        "#threads = []\n",
        "for link in all_article_list:\n",
        "  threads.append(threading.Thread(target=resultspress, args=(link,)))\n",
        "\n",
        "# Start all threads\n",
        "for x in threads:\n",
        "  x.start()\n",
        "  time.sleep(0.5)\n",
        "\n",
        " # Wait for all of them to finish\n",
        "for x in threads:\n",
        "  x.join()\n",
        "\n",
        "url = \"https://www.kia.ru/kiaflex/\"\n",
        "response = requests.get(url=url, headers=HEADERS).text\n",
        "html = ''.join(line.strip() for line in response.split(\"\\n\"))\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "savetxt('kiaflex', 'kiaflex.txt', '# '+parser(soup))\n",
        "\n",
        "#Тест ассортимента\n",
        "\n",
        "base_url = \"https://www.kia.ru/ajax/page/accessories/filter?sort=sort&order=desc&page=1\"\n",
        "\n",
        "\n",
        "headers = {\n",
        "    \"Referer\": \"https://www.kia.ru/service/accessories/\",  # Example Referer header\n",
        "}\n",
        "\n",
        "bigdata = []\n",
        "\n",
        "\n",
        "start_page = 1\n",
        "\n",
        "while True:\n",
        "    current_url = base_url + str(start_page)\n",
        "    response = requests.get(current_url, headers=headers)\n",
        "    json_data = response.text\n",
        "    data = json.loads(json_data)\n",
        "    if len(data['content']['accessories']) == 0: break\n",
        "\n",
        "\n",
        "\n",
        "    for tech in data['content']['accessories']:\n",
        "      bigdata.append(tech)\n",
        "\n",
        "    start_page += 1\n",
        "\n",
        "text = '# Аксесуары\\n'\n",
        "for i in bigdata:\n",
        "  if i['material'] == '':\n",
        "    i['material'] = {}\n",
        "    i['material']['name'] = ''\n",
        "  text += f'## Наименование: {i[\"name\"].strip()}, Фото: https://cdn.kia.ru/resize/1295x632{i[\"image\"]},'\n",
        "  text += f'Артикул: {i[\"article\"]}, Материал: {i[\"material\"][\"name\"].strip()}.\\n{i[\"text\"]}'\n",
        "  if i['technical_features'] is not None:\n",
        "    html = ''.join(line.strip() for line in i['technical_features'].split(\"\\n\"))\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    technical_features = '. '.join(soup.strings)\n",
        "  text += technical_features + '\\n'\n",
        "savetxt('accessories', 'accessories.txt', text)\n",
        "\n",
        "!echo \"# Технологии\" > database.txt\n",
        "!cat result/about/*.txt >> database.txt\n",
        "!cat result/press/*.txt >> database.txt\n",
        "!cat result/models/*.txt >> database.txt\n",
        "!cat result/kiaflex/*.txt >> database.txt\n",
        "!cat result/accessories/*.txt >> database.txt\n",
        "!zip -r result.zip result database.txt\n",
        "!zip database.zip database.txt"
      ]
    }
  ]
}