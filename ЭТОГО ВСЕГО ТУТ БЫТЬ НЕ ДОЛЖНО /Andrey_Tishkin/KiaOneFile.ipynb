{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "raTrjmB59OtT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install fake-useragent\n",
        "import datetime\n",
        "import numpy as np\n",
        "import openpyxl\n",
        "import re\n",
        "import requests\n",
        "import threading, time\n",
        "import json\n",
        "import os\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "def savetxt(dir, name, text):\n",
        "  if not os.path.exists('result'): os.mkdir('result')\n",
        "  if not os.path.exists('result/'+dir): os.mkdir('result/'+dir)\n",
        "  with open('result/'+dir+'/'+name,'w',encoding='utf-8') as f:\n",
        "    f.write(text)\n",
        "  return True\n",
        "\n",
        "def resulttotext(r):\n",
        "  if r is not None:\n",
        "    return r.text+'.'\n",
        "  else:\n",
        "    return ''\n",
        "\n",
        "def parser(soup):\n",
        "    text = ''\n",
        "    pages = soup.findAll('div', class_='g-padding')\n",
        "    if pages is None:\n",
        "      print(soup.find('title').text.split(' – ')[0])\n",
        "      print(soup)\n",
        "    else:\n",
        "      for page in pages:\n",
        "        if page.find('div',class_='faq') is not None:\n",
        "          text += page.find('div',class_='faq').text\n",
        "        else:\n",
        "          content = page.select('div[class*=\"text-\"]')\n",
        "          text += ''.join(i.text+'\\n' for i in content)\n",
        "    return text\n",
        "\n",
        "# Pars page whith models\n",
        "\n",
        "pagemodels = requests.get('https://www.kia.ru/models/').text\n",
        "modellinks = []\n",
        "\n",
        "html = ''.join(line.strip() for line in pagemodels.split(\"\\n\"))\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "cards_list = soup.find_all('div', class_='car-card')\n",
        "\n",
        "for card in cards_list:\n",
        "  modellinks.append('https://www.kia.ru'+ str(card.a['href']))\n",
        "\n",
        "def getmodelsoup(url):\n",
        "  model = requests.get(url).text\n",
        "  html = ''.join(line.strip() for line in model.split(\"\\n\"))\n",
        "  return BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "modeldict = {}\n",
        "\n",
        "for i in modellinks:\n",
        "  modeldict[i] = getmodelsoup(i)\n",
        "\n",
        "def stringstolist(div):\n",
        "  '''Превращаем блок в список строк'''\n",
        "  lis = []\n",
        "  try:\n",
        "    for string in div.strings:\n",
        "      if string not in [' ','']:\n",
        "        lis.append(string.text)\n",
        "  except:\n",
        "    lis = ['']\n",
        "  return lis\n",
        "\n",
        "\n",
        "def parsermodel(soup):\n",
        "  text = ''\n",
        "\n",
        "  # Обрабатываю по id basic\n",
        "  basic = soup.select('div[id*=\"basic_\"]')\n",
        "  for div in basic:\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)\n",
        "    try:\n",
        "      img = div.find('img')['data-src']\n",
        "    except:\n",
        "      img = ''\n",
        "    text += f'## {textlist[0]}\\n{textlist[0]}\\n{\". \".join(textlist[1:])}\\nФото {img}\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id dizain\n",
        "  dizain = soup.select('div[id*=\"dizain_\"]')\n",
        "  for div in dizain:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id eksterer\n",
        "  eksterer = soup.select('div[id*=\"eksterer_\"]')\n",
        "  for div in eksterer:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id interer\n",
        "  interer = soup.select('div[id*=\"interer_\"]')\n",
        "  for div in interer:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id style\n",
        "  style = soup.select('div[id*=\"style_\"]')\n",
        "  for div in style:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id multimedia\n",
        "  multimedia = soup.select('div[id*=\"multimedia_\"]')\n",
        "  for div in multimedia:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id tehnologii\n",
        "  tehnologii = soup.select('div[id*=\"tehnologii_\"]')\n",
        "  for div in tehnologii:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id bezopasnost\n",
        "  bezopasnost = soup.select('div[id*=\"bezopasnost_\"]')\n",
        "  for div in bezopasnost:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id teplye_opcii\n",
        "  teplye_opcii = soup.select('div[id*=\"teplye_opcii_\"]')\n",
        "  for div in teplye_opcii:\n",
        "    if len(div['id'].split('_')) > 3: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id komfort\n",
        "  komfort = soup.select('div[id*=\"komfort_\"]')\n",
        "  for div in komfort:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id vmestimost\n",
        "  vmestimost = soup.select('div[id*=\"vmestimost_\"]')\n",
        "  for div in vmestimost:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{textlist[0]}\\n{textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    for t in textlist[3:]:\n",
        "      text += t+'. '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  # Обрабатываю по id dvigatel\n",
        "  dvigatel = soup.select('div[id*=\"dvigatel_\"]')\n",
        "  for div in dvigatel:\n",
        "    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки\n",
        "    # Перевожу строки в список, для удобного обращиния\n",
        "    textlist = stringstolist(div)[:-1]\n",
        "    text += f'## {textlist[0]}\\n{textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "    text += 'Модели двигателей: '\n",
        "    ul = div.find('ul')\n",
        "    for t in stringstolist(ul):\n",
        "      text += t+', '\n",
        "    text = text[:-2]+'\\n\\n'\n",
        "\n",
        "  return text\n",
        "\n",
        "def parseroptions(url):\n",
        "  text = '## Комплектации\\nКомплектации\\n'\n",
        "  links = []\n",
        "  url = url.replace('desc','options')\n",
        "  suop = getmodelsoup(url)\n",
        "  ahrefs = suop.findAll('div',class_='config__variants__slide')\n",
        "  for a in ahrefs:\n",
        "    cont = a.find('li').text\n",
        "    url = a.find('a')\n",
        "    option = getmodelsoup('https://www.kia.ru'+url['href'])\n",
        "    titel = option.find('title').text\n",
        "    text += f'### Комплектация: {titel}\\nКомплектация: {titel}\\nЦена: {cont}\\n'\n",
        "    info = option.findAll('div', class_=\"info-section\")\n",
        "\n",
        "    for i in info:\n",
        "      t2 = i.find('div', class_=\"info-section__header\").text\n",
        "      text += t2 + ': '\n",
        "      if t2.strip() == 'Технические характеристики' or t2.strip() == 'Спецификация':\n",
        "        dl = i.findAll('dl')\n",
        "        for j in dl:\n",
        "          text += j.find('dt').text + ': ' + j.find('dd').text + '; '\n",
        "      else:\n",
        "        li = i.findAll('li')\n",
        "        for j in li:\n",
        "          text += j.text + ', '\n",
        "      text = text[:-2] + '\\n'\n",
        "\n",
        "  return text\n",
        "\n",
        "def resultsmodel(link):\n",
        "\n",
        "  print(link)\n",
        "  name = modeldict[link].find('title').text.split(' – ')[0]\n",
        "  name = name.replace('/','-')\n",
        "  text = f'#  {name} - [link {link}]\\n{name} - [link {link}]\\n'\n",
        "  text += parsermodel(modeldict[link])\n",
        "  text += parseroptions(link)\n",
        "  savetxt('models', name+'.txt', text)\n",
        "  print('Done '+ name)\n",
        "\n",
        "\n",
        "threads = []\n",
        "# Добавляю потоки с функцией сохранения в файл в список потоков\n",
        "for link in modellinks:\n",
        "  threads.append(threading.Thread(target=resultsmodel, args=(link,)))\n",
        "\n",
        "# Технологии\n",
        "\n",
        "def parserabout(soup):\n",
        "    text = '## '\n",
        "    pages = soup.find('div', class_='articles-detail__technology-txt')\n",
        "    if pages is None:\n",
        "      print(soup.find('title').text.split(' – ')[0])\n",
        "      print(soup)\n",
        "    else:\n",
        "        text+=soup.find('title').text.split(' – ')[0]+ '\\n'\n",
        "        content = pages.select('div[class*=\"text-\"]')\n",
        "        text += ''.join(i.text+'\\n' for i in content)\n",
        "    return text\n",
        "\n",
        "url = \"https://www.kia.ru/ajax/page/technologies/more?limit=45&page=1\"\n",
        "\n",
        "headers = {\n",
        "    \"Referer\": \"https://www.kia.ru/about/technologies/\",  # Example Referer header\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "json_data = response.text\n",
        "\n",
        "data = json.loads(json_data)\n",
        "\n",
        "ids = [tech['id'] for tech in data['content']['technologies']]\n",
        "\n",
        "static_url = 'https://www.kia.ru/about/technologies/'\n",
        "urls = []\n",
        "\n",
        "for id in ids:\n",
        "    urls.append(static_url+id)\n",
        "\n",
        "def resultsabout(link):\n",
        "\n",
        "  print(link)\n",
        "  content = requests.get(link)\n",
        "  if content.status_code == 200:\n",
        "    #constructor-block\n",
        "    text = ''\n",
        "    html = ''.join(line.strip() for line in content.text.split(\"\\n\"))\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    name = soup.find('title').text.split(' – ')[0]\n",
        "    text += parserabout(soup)\n",
        "  savetxt('about', name.replace('/','-')+'.txt', text)\n",
        "  print('Done '+ name)\n",
        "print(urls)\n",
        "for link in urls:\n",
        "\n",
        "# Добавляю потоки с функцией сохранения в файл в список потоков\n",
        "  threads.append(threading.Thread(target=resultsabout, args=(link,)))\n",
        "\n",
        "# Сбор с закладки \"Журнал\"\n",
        "\n",
        "url = \"https://www.kia.ru/ajax/page/mediacenter/magazine/more?limit=100&page=1\"\n",
        "static_url = \"https://www.kia.ru/press/magazine/\"\n",
        "HEADERS = {\"Referer\": static_url}\n",
        "\n",
        "response = requests.get(url=url, headers=HEADERS)\n",
        "json_data = response.text\n",
        "data = json.loads(json_data)\n",
        "all_article_list = []\n",
        "\n",
        "for article in data[\"content\"][\"media_center\"][\"magazine\"]:\n",
        "    code = article[\"code\"]\n",
        "    all_article_list.append(static_url + code + '/')\n",
        "\n",
        "url = \"https://www.kia.ru/ajax/page/mediacenter/news/more?limit=100&page=1\"\n",
        "static_url = \"https://www.kia.ru/press/news/\"\n",
        "response = requests.get(url=url, headers=HEADERS)\n",
        "json_data = response.text\n",
        "data = json.loads(json_data)\n",
        "all_article_list = []\n",
        "\n",
        "for article in data[\"content\"][\"media_center\"][\"news\"]:\n",
        "    code = article[\"code\"]\n",
        "    all_article_list.append(static_url + code + '/')\n",
        "\n",
        "def resultspress(link):\n",
        "\n",
        "  print(link)\n",
        "  content = requests.get(link)\n",
        "  if content.status_code == 200:\n",
        "    #constructor-block\n",
        "    text = ''\n",
        "    html = ''.join(line.strip() for line in content.text.split(\"\\n\"))\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    name = soup.find('title').text.split(' – ')[0]\n",
        "    head_press = soup.h1.text.strip() #заголовок статьи\n",
        "    all_img_list, all_img = [], \"\"\n",
        "    try:\n",
        "      [all_img_list.append(img.find(\"img\").attrs.get(\"src\")) for img in soup.find_all(\"div\", class_=\"articles-detail__content__offset\")]  # все фото из статьи\n",
        "      for img in all_img_list:\n",
        "        all_img += img + \",\"\n",
        "    except:\n",
        "      all_img = \"\"\n",
        "\n",
        "    try:\n",
        "      date_press = soup.select_one(\"div.articles-detail__date\").text.strip()  # дата статьи\n",
        "      for val in soup.find_all(\"div\", class_=\"g-container\"):\n",
        "        for child in val.children:\n",
        "          if date_press in child.text:\n",
        "            text += f\"## {head_press}\\n{head_press}\\n\"\n",
        "            text += child.text.replace(\"\\xa0\", \" \")\n",
        "            text += f\"\\nФотографии из статьи: {all_img[:-1]}\\n\\n\"\n",
        "    except:\n",
        "      for val in soup.find_all(\"div\", class_=\"g-container\"):\n",
        "        for child in val.children:\n",
        "          if child.find(\"h1\") is not None and child.find(\"h1\") != -1:\n",
        "            text += f\"## {head_press}\\n{head_press}\\n\"\n",
        "            text += child.text.replace(\"\\xa0\", \" \")\n",
        "            text += f\"\\nФотографии из статьи: {all_img[:-1]}\\n\\n\"\n",
        "    if text != '':\n",
        "      savetxt('press', name.replace('/','-')+'.txt', text)\n",
        "      print('Done '+ name)\n",
        "#threads = []\n",
        "for link in all_article_list:\n",
        "  threads.append(threading.Thread(target=resultspress, args=(link,)))\n",
        "\n",
        "# Start all threads\n",
        "for x in threads:\n",
        "  x.start()\n",
        "  time.sleep(0.5)\n",
        "\n",
        " # Wait for all of them to finish\n",
        "for x in threads:\n",
        "  x.join()\n",
        "\n",
        "url = \"https://www.kia.ru/kiaflex/\"\n",
        "response = requests.get(url=url, headers=HEADERS).text\n",
        "html = ''.join(line.strip() for line in response.split(\"\\n\"))\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "savetxt('kiaflex', 'kiaflex.txt', '# kiaflex\\n'+parser(soup))\n",
        "\n",
        "#Тест ассортимента\n",
        "\n",
        "base_url = \"https://www.kia.ru/ajax/page/accessories/filter?sort=sort&order=desc&page=1\"\n",
        "\n",
        "\n",
        "headers = {\n",
        "    \"Referer\": \"https://www.kia.ru/service/accessories/\",  # Example Referer header\n",
        "}\n",
        "\n",
        "bigdata = []\n",
        "\n",
        "\n",
        "start_page = 1\n",
        "\n",
        "while True:\n",
        "    current_url = base_url + str(start_page)\n",
        "    response = requests.get(current_url, headers=headers)\n",
        "    json_data = response.text\n",
        "    data = json.loads(json_data)\n",
        "    if len(data['content']['accessories']) == 0: break\n",
        "\n",
        "    for tech in data['content']['accessories']:\n",
        "      bigdata.append(tech)\n",
        "\n",
        "    start_page += 1\n",
        "\n",
        "text = '# Аксесуары\\nАксесуары\\n'\n",
        "for i in bigdata:\n",
        "  if i['material'] == '':\n",
        "    i['material'] = {}\n",
        "    i['material']['name'] = ''\n",
        "  text += f'## Наименование: {i[\"name\"].strip()} \\nНаименование: {i[\"name\"].strip()}, Фото: https://cdn.kia.ru/resize/1295x632{i[\"image\"]},'\n",
        "  text += f'Артикул: {i[\"article\"]}, Материал: {i[\"material\"][\"name\"].strip()}.\\n{i[\"text\"]}'\n",
        "  if i['technical_features'] is not None:\n",
        "    html = ''.join(line.strip() for line in i['technical_features'].split(\"\\n\"))\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    technical_features = '. '.join(soup.strings)\n",
        "  text += technical_features + '\\n'\n",
        "savetxt('accessories', 'accessories.txt', text)\n",
        "\n",
        "def textoil(need: bool = False):\n",
        "  if need:\n",
        "    oil_res = {}\n",
        "    headers={\"Referer\": 'https://www.kia.ru/'}\n",
        "\n",
        "    json_data = json.loads(requests.get('https://www.kia.ru/ajax/decoder/model_lines',\n",
        "                            headers={\"Referer\": 'https://www.kia.ru/'}).text)\n",
        "    model_lines = list(json_data['content']['model_lines'])\n",
        "    print(datetime.datetime.now())\n",
        "    for id in model_lines:\n",
        "      try:\n",
        "        model_line_id = id['id']\n",
        "        json_data = json.loads(requests.get('https://www.kia.ru/ajax/decoder/years',\n",
        "                                  params={'model_line_id': model_line_id},\n",
        "                                  headers=headers).text)\n",
        "        years = list(json_data['content']['years'])\n",
        "        for year in years:\n",
        "            year = year['id']\n",
        "            json_data = json.loads(requests.get('https://www.kia.ru/ajax/decoder/models',\n",
        "                                    params={'model_line_id': model_line_id, 'year': year},\n",
        "                                    headers=headers).text)\n",
        "            models = list(json_data['content']['models'])\n",
        "            for model in models:\n",
        "              model_id = model['id']\n",
        "              json_data = json.loads(requests.get('https://www.kia.ru/ajax/decoder/complectations',\n",
        "                                    params={'model_id': model_id},\n",
        "                                    headers=headers).text)\n",
        "              complectations = list(json_data['content']['complectations'])\n",
        "              for complectation in complectations:\n",
        "                complectation_id = complectation['id']\n",
        "                json_data = json.loads(\n",
        "                              requests.get('https://www.kia.ru/ajax/page/oils/complectations/'+complectation_id,\n",
        "                              headers=headers).text)\n",
        "                oils = list(json_data['content']['oils'])\n",
        "                car = json_data['content']['car']\n",
        "                if oils != []:\n",
        "                  for oil in oils:\n",
        "                    com_name = car['model']['model_line']['name'] + ' '\n",
        "                    com_name += car['model']['generation']['name'] +'/ '+str(year)+'/ '\n",
        "                    com_name += car['model']['carcass']['name'] + '/ '\n",
        "                    com_name += str(car['modification']['engine']['engine_volume']) + ' '\n",
        "                    com_name += car['modification']['engine']['engine_type'] + '/ '\n",
        "                    com_name += car['modification']['engine']['fuel_type'] + '/ '\n",
        "                    com_name += car['modification']['transmission']['drive'] + '/ '\n",
        "                    com_name += car['modification']['transmission']['gearbox'] + '/ '\n",
        "\n",
        "                    if not oil['name'] in oil_res.keys():\n",
        "                      oil_res[oil['name']] = [oil['description'],com_name]\n",
        "                    else:\n",
        "                      oil_res[oil['name']] += [com_name]\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "    print(datetime.datetime.now())\n",
        "    text = '# Масло для двигателя\\nМасло для двигателя\\n'\n",
        "    for oil in oil_res:\n",
        "      text += '## Наименование: '+ oil.replace('#','№')+'\\n'\n",
        "      text += oil.replace('#','№')+'\\n'+oil_res[oil][0]+'\\n'\n",
        "      text += 'Подходит моделям с комплектациями: ' + ', '.join(oil_res[oil][1:]) +'\\n'\n",
        "    return text\n",
        "  else:\n",
        "    return requests.get('https://github.com/terrainternship/KIA-GPT/raw/main/knowledge/oils.txt').text\n",
        "\n",
        "savetxt('oil', 'oils.txt', textoil())\n",
        "\n",
        "# спецпредложения\n",
        "url = \"https://www.kia.ru/service/special/\"\n",
        "savetxt('special', 'special.txt', '')\n",
        "try:\n",
        "    ua = UserAgent().random\n",
        "    HEADERS = {\"user-agent\": ua, \"accept\": \"*/*\"}\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        HEAD = soup.h1.text.strip() #заголовок главный\n",
        "        with open(\"result/special/special.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(f\"# {HEAD}\\n{HEAD}\\n\")\n",
        "\n",
        "        list_special = []\n",
        "        [list_special.append(\"https://www.kia.ru\" + val.get(\"href\")) for val in soup.find_all(\"a\", class_=\"articles-item__link\")]\n",
        "\n",
        "        list_date = []\n",
        "        [list_date.append(\"Дата действия спецпредложения: \" + val.text.replace(\"\\n\", \"\")) for val in soup.find_all(\"div\", class_=\"articles-item__date\")]\n",
        "\n",
        "        for i in range(len(list_special)):\n",
        "            try:\n",
        "                response = requests.get(url=list_special[i], headers=HEADERS)\n",
        "                if response.status_code == 200:\n",
        "                    html = ''.join(line.strip() for line in response.text.split(\"\\n\"))\n",
        "                    soup = BeautifulSoup(html, \"html.parser\")\n",
        "                    head_special = soup.h1.text.strip() #заголовок статьи\n",
        "                    header_text = soup.find(\"div\", class_=\"special-offer__header-text-content\").text\n",
        "\n",
        "                    all_img_list, all_img = [], \"\"\n",
        "                    try:\n",
        "                        [all_img_list.append(img.find(\"img\").attrs.get(\"src\")) for img in soup.find_all(\"div\", class_=\"special-offer-detail__image\")]  # все фото из статьи\n",
        "                        for img in all_img_list:\n",
        "                            all_img += img + \",\"\n",
        "                    except:\n",
        "                        all_img = \"\"\n",
        "\n",
        "                    text_special = \"\"\n",
        "                    for val in soup.find_all(\"div\", class_=\"special-offer-detail\"):\n",
        "                        for child in val.children:\n",
        "                            text_special += child.text + \"\\n\"\n",
        "\n",
        "                    with open(\"result/special/special.txt\", \"a\", encoding=\"utf-8\") as file:\n",
        "                        file.write(f\"## {head_special}\\n{head_special}\\n\")\n",
        "                        file.write(f\"{list_date[i]}\\n\")\n",
        "                        file.write(f\"{header_text}\\n\")\n",
        "                        file.write(text_special) #.replace(\"\\xa0\", \" \")\n",
        "                        file.write(f\"\\nФотографии из статьи: {all_img[:-1]}\\n\\n\")\n",
        "\n",
        "            except ConnectionError:\n",
        "                print(\"Проверьте подключение к сети\")\n",
        "\n",
        "except ConnectionError:\n",
        "    print(\"Проверьте подключение к сети\")\n",
        "\n",
        "!echo \"# Технологии\" > database.txt\n",
        "!echo \"Технологии\" >> database.txt\n",
        "!cat result/about/*.txt >> database.txt\n",
        "!cat result/models/*.txt >> database.txt\n",
        "!cat result/accessories/*.txt >> database.txt\n",
        "!cat result/special/*.txt >> database.txt\n",
        "!cat result/oil/*.txt >> database.txt\n",
        "!cat result/kiaflex/*.txt >> database.txt\n",
        "!cat result/press/*.txt >> database.txt\n",
        "\n",
        "!zip -r result.zip result database.txt\n",
        "!zip database.zip database.txt"
      ]
    }
  ]
}