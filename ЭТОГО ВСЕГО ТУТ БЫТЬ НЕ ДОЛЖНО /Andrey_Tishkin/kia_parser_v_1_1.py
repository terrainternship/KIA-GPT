# -*- coding: utf-8 -*-
"""KIA_v_1.1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jt_qCenuKMSdTvIG9hHht_YNF4Y0eLvi
"""

import numpy as np
import openpyxl
import re
import requests
import threading, time
import json
import os

from bs4 import BeautifulSoup

def savetxt(dir, name, text):
  if not os.path.exists('result'): os.mkdir('result')
  if not os.path.exists('result/'+dir): os.mkdir('result/'+dir)
  if os.path.exists('result/'+dir+'/'+name):
    print('Файл существует')
    return None
  with open('result/'+dir+'/'+name,'w',encoding='utf-8') as f:
    f.write(text)
  return True

def resulttotext(r):
  if r is not None:
    return r.text+'.'
  else:
    return ''

def parser(soup):
    text = ''
    pages = soup.findAll('div', class_='g-padding')
    if pages is None:
      print(soup.find('title').text.split(' – ')[0])
      print(soup)
    else:
      for page in pages:
        if page.find('div',class_='faq') is not None:
          text += page.find('div',class_='faq').text
        else:
          content = page.select('div[class*="text-"]')
          text += ''.join(i.text+'\n' for i in content)
    return text

# Pars page whith models

pagemodels = requests.get('https://www.kia.ru/models/').text
modellinks = []

html = ''.join(line.strip() for line in pagemodels.split("\n"))
soup = BeautifulSoup(html, "html.parser")

cards_list = soup.find_all('div', class_='car-card')

for card in cards_list:
  modellinks.append('https://www.kia.ru'+ str(card.a['href']))

def getmodelsoup(url):
  model = requests.get(url).text
  html = ''.join(line.strip() for line in model.split("\n"))
  return BeautifulSoup(html, "html.parser")

modeldict = {}

for i in modellinks:
  modeldict[i] = getmodelsoup(i)

def stringstolist(div):
  '''Превращаем блок в список строк'''
  lis = []
  try:
    for string in div.strings:
      if string not in [' ','']:
        lis.append(string.text)
  except:
    lis = ['']
  return lis


def parsermodel(soup):
  text = ''

  # Обрабатываю по id basic
  basic = soup.select('div[id*="basic_"]')
  for div in basic:
    # Перевожу строки в список, для удобного обращиния
    textlist = stringstolist(div)
    try:
      img = div.find('img')['data-src']
    except:
      img = ''
    text += f'## {textlist[0]}\n{". ".join(textlist[1:])}\n[IMG {img}]\n\n'

  # Обрабатываю по id dizain
  dizain = soup.select('div[id*="dizain_"]')
  for div in dizain:
    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки
    # Перевожу строки в список, для удобного обращиния
    textlist = stringstolist(div)[:-1]
    text += f'## {textlist[0]}\n{". ".join(textlist[1:3])}\n'
    for t in textlist[3:]:
      text += t+'. '
    text = text[:-2]+'\n\n'

  # Обрабатываю по id eksterer
  eksterer = soup.select('div[id*="eksterer_"]')
  for div in eksterer:
    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки
    # Перевожу строки в список, для удобного обращиния
    textlist = stringstolist(div)[:-1]
    text += f'## {textlist[0]}\n{". ".join(textlist[1:3])}\n'
    for t in textlist[3:]:
      text += t+'. '
    text = text[:-2]+'\n\n'

  # Обрабатываю по id interer
  interer = soup.select('div[id*="interer_"]')
  for div in interer:
    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки
    # Перевожу строки в список, для удобного обращиния
    textlist = stringstolist(div)[:-1]
    text += f'## {textlist[0]}\n{". ".join(textlist[1:3])}\n'
    for t in textlist[3:]:
      text += t+'. '
    text = text[:-2]+'\n\n'

  # Обрабатываю по id style
  style = soup.select('div[id*="style_"]')
  for div in style:
    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки
    # Перевожу строки в список, для удобного обращиния
    textlist = stringstolist(div)[:-1]
    text += f'## {textlist[0]}\n{". ".join(textlist[1:3])}\n'
    for t in textlist[3:]:
      text += t+'. '
    text = text[:-2]+'\n\n'

  # Обрабатываю по id multimedia
  multimedia = soup.select('div[id*="multimedia_"]')
  for div in multimedia:
    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки
    # Перевожу строки в список, для удобного обращиния
    textlist = stringstolist(div)[:-1]
    text += f'## {textlist[0]}\n{". ".join(textlist[1:3])}\n'
    for t in textlist[3:]:
      text += t+'. '
    text = text[:-2]+'\n\n'

  # Обрабатываю по id tehnologii
  tehnologii = soup.select('div[id*="tehnologii_"]')
  for div in tehnologii:
    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки
    # Перевожу строки в список, для удобного обращиния
    textlist = stringstolist(div)[:-1]
    text += f'## {textlist[0]}\n{". ".join(textlist[1:3])}\n'
    for t in textlist[3:]:
      text += t+'. '
    text = text[:-2]+'\n\n'

  # Обрабатываю по id bezopasnost
  bezopasnost = soup.select('div[id*="bezopasnost_"]')
  for div in bezopasnost:
    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки
    # Перевожу строки в список, для удобного обращиния
    textlist = stringstolist(div)[:-1]
    text += f'## {textlist[0]}\n{". ".join(textlist[1:3])}\n'
    for t in textlist[3:]:
      text += t+'. '
    text = text[:-2]+'\n\n'

  # Обрабатываю по id teplye_opcii
  teplye_opcii = soup.select('div[id*="teplye_opcii_"]')
  for div in teplye_opcii:
    if len(div['id'].split('_')) > 3: continue # Пропускаем внутренние блоки
    # Перевожу строки в список, для удобного обращиния
    textlist = stringstolist(div)[:-1]
    text += f'## {textlist[0]}\n{". ".join(textlist[1:3])}\n'
    for t in textlist[3:]:
      text += t+'. '
    text = text[:-2]+'\n\n'

  # Обрабатываю по id komfort
  komfort = soup.select('div[id*="komfort_"]')
  for div in komfort:
    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки
    # Перевожу строки в список, для удобного обращиния
    textlist = stringstolist(div)[:-1]
    text += f'## {textlist[0]}\n{". ".join(textlist[1:3])}\n'
    for t in textlist[3:]:
      text += t+'. '
    text = text[:-2]+'\n\n'

  # Обрабатываю по id vmestimost
  vmestimost = soup.select('div[id*="vmestimost_"]')
  for div in vmestimost:
    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки
    # Перевожу строки в список, для удобного обращиния
    textlist = stringstolist(div)[:-1]
    text += f'## {textlist[0]}\n{". ".join(textlist[1:3])}\n'
    for t in textlist[3:]:
      text += t+'. '
    text = text[:-2]+'\n\n'

  # Обрабатываю по id dvigatel
  dvigatel = soup.select('div[id*="dvigatel_"]')
  for div in dvigatel:
    if len(div['id'].split('_')) > 2: continue # Пропускаем внутренние блоки
    # Перевожу строки в список, для удобного обращиния
    textlist = stringstolist(div)[:-1]
    text += f'## {textlist[0]}\n{". ".join(textlist[1:3])}\n'
    text += 'Модели двигателей: '
    ul = div.find('ul')
    for t in stringstolist(ul):
      text += t+', '
    text = text[:-2]+'\n\n'

  return text

def parseroptions(url):
  text = '## Комплектации\n'
  links = []
  url = url.replace('desc','options')
  suop = getmodelsoup(url)
  ahrefs = suop.findAll('div',class_='config__variants__slide')
  for a in ahrefs:
    cont = a.find('li').text
    url = a.find('a')
    option = getmodelsoup('https://www.kia.ru'+url['href'])
    titel = option.find('title').text
    text += f'### Комплектация: {titel}\nЦена: {cont}\n'
    info = option.findAll('div', class_="info-section")

    for i in info:
      t2 = i.find('div', class_="info-section__header").text
      text += t2 + ': '
      if t2.strip() == 'Технические характеристики' or t2.strip() == 'Спецификация':
        dl = i.findAll('dl')
        for j in dl:
          text += j.find('dt').text + ': ' + j.find('dd').text + '; '
      else:
        li = i.findAll('li')
        for j in li:
          text += j.text + ', '
      text = text[:-2] + '\n'

  return text

def resultsmodel(link):

  print(link)
  name = modeldict[link].find('title').text.split(' – ')[0]
  name = name.replace('/','-')
  text = f'#  {name} - [link {link}]\n'
  text += parsermodel(modeldict[link])
  text += parseroptions(link)
  savetxt('models', name+'.txt', text)
  print('Done '+ name)


threads = []
# Добавляю потоки с функцией сохранения в файл в список потоков
for link in modellinks:
  threads.append(threading.Thread(target=resultsmodel, args=(link,)))

# Технологии

def parserabout(soup):
    text = '## '
    pages = soup.find('div', class_='articles-detail__technology-txt')
    if pages is None:
      print(soup.find('title').text.split(' – ')[0])
      print(soup)
    else:
        content = pages.select('div[class*="text-"]')
        text += ''.join(i.text+'\n' for i in content)
    return text

url = "https://www.kia.ru/ajax/page/technologies/more?limit=45&page=1"

headers = {
    "Referer": "https://www.kia.ru/about/technologies/",  # Example Referer header
}

response = requests.get(url, headers=headers)
json_data = response.text

data = json.loads(json_data)

ids = [tech['id'] for tech in data['content']['technologies']]

static_url = 'https://www.kia.ru/about/technologies/'
urls = []

for id in ids:
    urls.append(static_url+id)

def resultsabout(link):

  print(link)
  content = requests.get(link)
  if content.status_code == 200:
    #constructor-block
    text = ''
    html = ''.join(line.strip() for line in content.text.split("\n"))
    soup = BeautifulSoup(html, "html.parser")
    name = soup.find('title').text.split(' – ')[0]
    text += parserabout(soup)
  savetxt('about', name.replace('/','-')+'.txt', text)
  print('Done '+ name)
print(urls)
for link in urls:

# Добавляю потоки с функцией сохранения в файл в список потоков
  threads.append(threading.Thread(target=resultsabout, args=(link,)))

# Start all threads
for x in threads:
  x.start()
  time.sleep(0.5)

 # Wait for all of them to finish
for x in threads:
  x.join()

# Сбор с закладки "Журнал"

url = "https://www.kia.ru/ajax/page/mediacenter/magazine/more?limit=100&page=1"
static_url = "https://www.kia.ru/press/magazine/"
HEADERS = {"Referer": static_url}

response = requests.get(url=url, headers=HEADERS)
json_data = response.text
data = json.loads(json_data)
all_article_list = []

for article in data["content"]["media_center"]["magazine"]:
    code = article["code"]
    all_article_list.append(static_url + code + '/')

for link in all_article_list:

    response = requests.get(url=link, headers=HEADERS)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, "html.parser")
        head_press = soup.h1.text.strip() #заголовок статьи
        all_img_list, all_img = [], ""
        try:
            [all_img_list.append(img.find("img").attrs.get("src")) for img in soup.find_all("div", class_="articles-detail__content__offset")]  # все фото из статьи
            for img in all_img_list:
                all_img += img + ","
        except:
            all_img = ""

        try:
            date_press = soup.select_one("div.articles-detail__date").text.strip()  # дата статьи
            for val in soup.find_all("div", class_="g-container"):
                for child in val.children:
                    if date_press in child.text:
                        with open("press.txt", "a", encoding="utf-8") as file:
                            file.write(f"## {head_press}>\n")
                            file.write(child.text.replace("\xa0", " "))
                            file.write(f"\n\nФотографии из статьи: {all_img[:-1]}\n\n")
        except:
            for val in soup.find_all("div", class_="g-container"):
                for child in val.children:
                    if child.find("h1") is not None and child.find("h1") != -1:
                        with open("press.txt", "a", encoding="utf-8") as file:
                            file.write(f"## {head_press}\n")
                            file.write(child.text.replace("\xa0", " "))
                            file.write(f"\nФотографии из статьи: {all_img[:-1]}\n\n")


# Сбор с закладки "Новости" (последние 100)

url = "https://www.kia.ru/ajax/page/mediacenter/news/more?limit=100&page=1"
static_url = "https://www.kia.ru/press/news/"
HEADERS = {"Referer": static_url}

response = requests.get(url=url, headers=HEADERS)
json_data = response.text
data = json.loads(json_data)
all_article_list = []

for article in data["content"]["media_center"]["news"]:
    code = article["code"]
    all_article_list.append(static_url + code + '/')

for link in all_article_list:

    response = requests.get(url=link, headers=HEADERS)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, "html.parser")
        head_press = soup.h1.text.strip() #заголовок статьи
        all_img_list, all_img = [], ""
        try:
            [all_img_list.append(img.find("img").attrs.get("src")) for img in soup.find_all("div", class_="articles-detail__content__offset")]  # все фото из статьи
            for img in all_img_list:
                all_img += img + ","
        except:
            all_img = ""

        try:
            date_press = soup.select_one("div.articles-detail__date").text.strip()  # дата статьи
            for val in soup.find_all("div", class_="g-container"):
                for child in val.children:
                    if date_press in child.text:
                        with open("news.txt", "a", encoding="utf-8") as file:
                            file.write(f"## {head_press}>\n")
                            file.write(child.text.replace("\xa0", " "))
                            file.write(f"\n\nФотографии из статьи: {all_img[:-1]}\n\n")
        except:
            for val in soup.find_all("div", class_="g-container"):
                for child in val.children:
                    if child.find("h1") is not None and child.find("h1") != -1:
                        with open("news.txt", "a", encoding="utf-8") as file:
                            file.write(f"## {head_press}\n")
                            file.write(child.text.replace("\xa0", " "))
                            file.write(f"\nФотографии из статьи: {all_img[:-1]}\n\n")

#!echo "# Технологии" > result.txt
#!cat result/about/*.txt >> result.txt
#!cat result/models/*.txt >> result.txt
#!zip -r result.zip result result.txt