{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLWs9eLRC63S"
      },
      "source": [
        "# Установка"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wu61Uz2K__i6"
      },
      "outputs": [],
      "source": [
        "# @title Установка пакетов\n",
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install --upgrade tiktoken\n",
        "!pip install langchain openai\n",
        "!pip install faiss-cpu\n",
        "\n",
        "clear_output()\n",
        "\n",
        "import getpass\n",
        "import openai\n",
        "import os\n",
        "def get_key_ОpenAI():\n",
        "  openai.api_key = getpass.getpass(prompt='Введите секретный ключ для сервиса chatGPT: ')\n",
        "  os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
        "\n",
        "get_key_ОpenAI()\n",
        "\n",
        "# Для работы с Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVo7kzfodp-0"
      },
      "outputs": [],
      "source": [
        "#@title Импорт библиотек и Сервисные функции\n",
        "import gdown\n",
        "from IPython.display import clear_output\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.docstore.document import Document\n",
        "import requests\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "import pathlib\n",
        "import subprocess\n",
        "import tempfile\n",
        "\n",
        "import time\n",
        "import os\n",
        "import openai\n",
        "import tiktoken\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "import getpass\n",
        "\n",
        "\n",
        "# ----------------------------------\n",
        "MODEL_TURBO_16K = \"gpt-3.5-turbo-16k\"\n",
        "MODEL_TURBO_0613 = \"gpt-3.5-turbo-0613\"\n",
        "MODEL_GPT4 = \"gpt-4-0613\"\n",
        "# ----------------------------------\n",
        "\n",
        "clear_output()\n",
        "\n",
        "class WorkerОpenAI():\n",
        "  def __init__(self, \\\n",
        "               system_promt = \" \", \\\n",
        "               system_promt_lector = \" \", \\\n",
        "               mod = MODEL_TURBO_16K, \\\n",
        "               content_topics = None, \\\n",
        "               save_project = '/content/'):\n",
        "    self.model = mod\n",
        "    self.save_project  = save_project\n",
        "\n",
        "    if content_topics:\n",
        "      self.content_topics = self.load_txt_file(content_topics)\n",
        "\n",
        "    # системные настройки\n",
        "    self.system_promt = self.load_document_text(system_promt)\n",
        "    self.speaker_system_promt = self.load_document_text(system_promt_lector)\n",
        "\n",
        "\n",
        "  def load_document_text(self, url: str) -> str:\n",
        "      # функция для загрузки документа по ссылке из гугл док\n",
        "      match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)\n",
        "      if match_ is None:\n",
        "          raise ValueError('Invalid Google Docs URL')\n",
        "      doc_id = match_.group(1)\n",
        "      response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')\n",
        "      response.raise_for_status()\n",
        "      text = response.text\n",
        "      return text\n",
        "\n",
        "\n",
        "  def load_txt_file(self, file_path):\n",
        "      with open(file_path, 'r') as file_:\n",
        "          text = file_.read()\n",
        "      return text\n",
        "\n",
        "  # пример подсчета токенов\n",
        "  def num_tokens_from_messages(self, messages):\n",
        "      \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
        "      try:\n",
        "          encoding = tiktoken.encoding_for_model(self.model)\n",
        "      except KeyError:\n",
        "          encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "      # if self.model == \"gpt-3.5-turbo-0301\":  # note: future models may deviate from this\n",
        "      if self.model in [\"gpt-3.5-turbo-0613\", \"gpt-3.5-turbo-16k\", \"gpt-4-0613\"]:  # note: future models may deviate from this\n",
        "          num_tokens = 0\n",
        "          for message in messages:\n",
        "              num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
        "              for key, value in message.items():\n",
        "                  num_tokens += len(encoding.encode(value))\n",
        "                  if key == \"name\":  # if there's a name, the role is omitted\n",
        "                      num_tokens += -1  # role is always required and always 1 token\n",
        "          num_tokens += 2  # every reply is primed with <im_start>assistant\n",
        "          return num_tokens\n",
        "      else:\n",
        "          raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {self.model}.\n",
        "  See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
        "\n",
        "\n",
        "  def create_embedding_faiss_db(self, doc_txt_dir=\"/content/\", \\\n",
        "                                faiss_db_dir =\"/content/\", \\\n",
        "                                start_idx = 0, \\\n",
        "                                collection_name = \" \"):\n",
        "\n",
        "    def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "      \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "      encoding = tiktoken.get_encoding(encoding_name)\n",
        "      num_tokens = len(encoding.encode(string))\n",
        "      return num_tokens\n",
        "\n",
        "    # Для Копирайтера\n",
        "    self.splitter = RecursiveCharacterTextSplitter(['\\n\\n', '\\n', ' '], chunk_size=1024, chunk_overlap=300)\n",
        "    idx_file_folder = start_idx-1\n",
        "    chunkID = idx_file_folder\n",
        "\n",
        "    count_tokens = 0\n",
        "    # проходимся по всем данным\n",
        "    for _, file_ in enumerate(sorted(os.listdir(doc_txt_dir))):\n",
        "        print(\"Загружается файл: \", file_)\n",
        "        self.file_name = file_\n",
        "        idx_file_folder +=1\n",
        "        source_chunks = []\n",
        "        # разбиваем на несколько частей с помощью метода split_text\n",
        "        with open(doc_txt_dir + file_, \"r\") as f:\n",
        "          for chunk in self.splitter.split_text(f.read()):\n",
        "              chunkID += 1\n",
        "              source_chunks.append(Document(page_content=chunk, \\\n",
        "                                  metadata={'source': file_,\n",
        "                                            'chunkID': chunkID,\n",
        "                                            \"collection_name\": collection_name,\n",
        "                                            'idx_file_folder': idx_file_folder}))\n",
        "\n",
        "\n",
        "        # Создание индексов документа и СОХРАНЕНИЕ\n",
        "        # Если документ не пуст, то создать и сохранить базу индексов эмбеддингов отрезков документа\n",
        "        if len(source_chunks) > 0:\n",
        "            self.db = FAISS.from_documents(source_chunks, OpenAIEmbeddings())\n",
        "            count_token = num_tokens_from_string(' '.join([x.page_content for x in source_chunks]), \"cl100k_base\")\n",
        "            count_tokens += count_token\n",
        "            print('Количество токенов в документе :', count_token)\n",
        "            # print('ЦЕНА запроса:', 0.0004 * (count_token / 1000), ' $')\n",
        "\n",
        "            self.db.save_local(os.path.join(faiss_db_dir, collection_name, f\"{str(idx_file_folder)}_db_initial__{file_[:20]}\"))\n",
        "\n",
        "    print('\\nЦЕНА запроса создания базы индексов:', 0.0004 * (count_tokens / 1000), ' $')\n",
        "\n",
        "# ДЛЯ ОДНОГО КОНКРЕТНОГО ФАЙЛА\n",
        "  def create_embedding_one_file(self, doc_txt_dir=\"/content/\", \\\n",
        "                                file_ = \"Имя файла.txt\" , \\\n",
        "                                faiss_db_dir =\"/content/\"):\n",
        "\n",
        "    def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "      \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "      encoding = tiktoken.get_encoding(encoding_name)\n",
        "      num_tokens = len(encoding.encode(string))\n",
        "      return num_tokens\n",
        "\n",
        "    # Для Копирайтера\n",
        "    self.splitter = RecursiveCharacterTextSplitter(['\\n\\n', '\\n', ' '], chunk_size=1024, chunk_overlap=0)\n",
        "    chunkID = 0\n",
        "    count_tokens = 0\n",
        "    self.file_name = file_[:-3]\n",
        "    print(\"Загружается файл: \", file_)\n",
        "    # проходимся по всем данным\n",
        "    source_chunks = []\n",
        "    # разбиваем на несколько частей с помощью метода split_text\n",
        "    with open(os.path.join(doc_txt_dir, file_), \"r\") as f:\n",
        "      for chunk in self.splitter.split_text(f.read()):\n",
        "          chunkID += 1\n",
        "          source_chunks.append(Document(page_content=chunk, \\\n",
        "                              metadata={'source': file_,\n",
        "                                        'chunkID': chunkID}))\n",
        "\n",
        "    # Создание индексов документа и СОХРАНЕНИЕ\n",
        "    # Если документ не пуст, то создать и сохранить базу индексов эмбеддингов отрезков документа\n",
        "    if len(source_chunks) > 0:\n",
        "        self.db = FAISS.from_documents(source_chunks, OpenAIEmbeddings())\n",
        "        count_token = num_tokens_from_string(' '.join([x.page_content for x in source_chunks]), \"cl100k_base\")\n",
        "        count_tokens += count_token\n",
        "        print('Количество токенов в документе :', count_token)\n",
        "        # print('ЦЕНА запроса:', 0.0004 * (count_token / 1000), ' $')\n",
        "\n",
        "        self.db.save_local(os.path.join(faiss_db_dir, f\"db_initial__{self.file_name[:35]}\"))\n",
        "\n",
        "    print('\\nЦЕНА запроса создания базы индексов:', 0.0004 * (count_tokens / 1000), ' $')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ЗАПРОС в ChatGPT\n",
        "  def get_ChatCompletion(self, model,  # указываем модель\n",
        "                         messages,     # словарь запроса\n",
        "                         temp=0.2):    # температуру\n",
        "\n",
        "      completion = openai.ChatCompletion.create(\n",
        "        model = model,\n",
        "        messages = messages,\n",
        "        temperature = temp\n",
        "        )\n",
        "\n",
        "      # print(f'{completion[\"usage\"][\"total_tokens\"]} токенов использовано всего (вопрос-ответ).')\n",
        "      # print('ЦЕНА запроса с ответом :', 0.0015*(completion[\"usage\"][\"total_tokens\"]/1000), ' $')\n",
        "      # print('===========================================: \\n')\n",
        "      return completion.choices[0].message.content\n",
        "\n",
        "# ЗАПРОС на создание темы и подтемы по Материалам\n",
        "  def get_search_materials_topics_subtopics(self, materials, model_topics = \"gpt-3.5-turbo-16k\"):\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": f\"{self.system_promt}\"},\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"Проанализируй отрывок материала c семинара: {materials}.\n",
        "\n",
        "Выдели основные темы.\n",
        "Дай компактный, сжатый, обобщенный список тем и подтем.\n",
        "Темы Необходимо оформить _#, а подтемы оформить ##_.\n",
        "Используй только такой пример, ничего не добавляй лишнего.\n",
        "Пример составления списка:\n",
        "_#...\n",
        "##_...\n",
        "_#...\n",
        "##_...\n",
        "        \"\"\"}\n",
        "        ]\n",
        "\n",
        "    # example token count from the function defined above\n",
        "    # print(f\"{self.num_tokens_from_messages(messages=messages)} токенов использовано на вопрос \\n\")\n",
        "    try:\n",
        "      self.content_topics += self.get_ChatCompletion(model_topics, messages)\n",
        "    except:\n",
        "      print(\"Модель в настоящее время перегружена. Попробуйте позже.\")\n",
        "\n",
        "\n",
        "# ПОИСК ТЕМЫ и ПОДТЕМЫ\n",
        "  def search_topics_subtopics(self, num_chunk = 14):\n",
        "    self.content_topics = ''\n",
        "    materials = \"\"\n",
        "    # Выбираем блоки ПОДРЯД\n",
        "    len_chunk = len(self.db.docstore._dict)\n",
        "    if len_chunk < num_chunk + 1:\n",
        "        for _, doc in tqdm(self.db.docstore._dict.items()):\n",
        "            materials += f\"{doc.page_content}\\n\"\n",
        "        self.get_search_materials_topics_subtopics(materials)\n",
        "    else:\n",
        "        for ind, (key, doc) in tqdm(enumerate(self.db.docstore._dict.items())):\n",
        "          materials += f\"{doc.page_content}\\n\"\n",
        "          if (ind+1) % num_chunk == 0:\n",
        "            self.get_search_materials_topics_subtopics(materials)\n",
        "            materials = \"\"\n",
        "    if materials != \"\":\n",
        "        self.get_search_materials_topics_subtopics(materials)\n",
        "\n",
        "    print('Собрали список тем и подтем: ')\n",
        "    print(self.content_topics)\n",
        "\n",
        "    with open(f'{self.save_project}_{self.file_name[:20]}__Topic_Subtopic.txt', \"w\") as f:\n",
        "      f.write(self.content_topics)\n",
        "\n",
        "\n",
        "# ОБЪЕДИНЯЕМ схожие ТЕМЫ\n",
        "  def get_merge_topics(self, model_topics = \"gpt-3.5-turbo-16k\"):\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": f\"{self.system_promt}\"},\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"Проанализируй Темы и подтемы Семинара: {self.content_topics}.\n",
        "\n",
        "Необходимо объединить похожие по смыслу темы или подтемы, записать компактно.\n",
        "При необходимости перефразировать тему или подтему. Дай корректный список.\n",
        "Темы оформи _#, а подтемы оформи ##_.\n",
        "Используй только такой пример, ничего не добавляй лишнего.\n",
        "Пример составления списка:\n",
        "_#...\n",
        "##_...\n",
        "_#...\n",
        "##_...\n",
        "\"\"\"}\n",
        "]\n",
        "    # example token count from the function defined above\n",
        "    # print(f\"{self.num_tokens_from_messages(messages=messages)} токенов использовано на вопрос \\n\")\n",
        "    try:\n",
        "      self.content_topics = self.get_ChatCompletion(model_topics, messages)\n",
        "      print('Итоговый список тем и подтем: ')\n",
        "      print(self.content_topics)\n",
        "      with open(f'{self.save_project}_{self.file_name[:20]}__Topic_Subtopic_Final.txt', \"w\") as f:\n",
        "        f.write(self.content_topics)\n",
        "    except:\n",
        "      print(\"Модель в настоящее время перегружена. Попробуйте позже.\")\n",
        "\n",
        "# ОРГАНИЗУЕМ текст блоками\n",
        "  def organize_text(self, topic, num_chunks, model_topics):\n",
        "\n",
        "    # Выборка документов по схожести с подтемой\n",
        "    docs = self.db.similarity_search(topic, k = num_chunks)\n",
        "    message_content = '\\n'.join([doc.page_content + '\\n' for i, doc in enumerate(docs)])\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": f\"{self.speaker_system_promt}\"},\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"Вот отрывки Вашей лекции:\\n{message_content}\n",
        "\n",
        "Опираясь только на информацию с Лекции, указанной выше, расскажите подробнее по Теме: {topic}.\n",
        "\"\"\"}\n",
        "      ]\n",
        "\n",
        "    # example token count from the function defined above\n",
        "    # print('\\n ===========================================: ')\n",
        "    # print(f\"{self.num_tokens_from_messages(messages=messages)} токенов использовано на вопрос \\n\")\n",
        "    try:\n",
        "      self.final_text += self.get_ChatCompletion(model_topics, messages)\n",
        "    except:\n",
        "      print(\"Модель в настоящее время перегружена. Попробуйте позже.\")\n",
        "\n",
        "# ОРГАНИЗУЕМ ИТОГОВЫЙ ТЕКСТ\n",
        "  def organize_final_text(self, num_chunks = 8,\n",
        "                          model_topics = \"gpt-3.5-turbo-16k\",\n",
        "                          name = None,\n",
        "                          db_path = None):\n",
        "    if name:\n",
        "      self.file_name = name\n",
        "\n",
        "    if db_path:\n",
        "      self.db = FAISS.load_local(db_path, OpenAIEmbeddings())\n",
        "\n",
        "    # if db_path:\n",
        "    #   for curr_base in os.listdir(db_path):\n",
        "    #       self.db = FAISS.load_local(os.path.join(db_path, curr_base), OpenAIEmbeddings())\n",
        "\n",
        "    self.final_text = \"\"\n",
        "    list_topics = self.content_topics.split('\\n')\n",
        "    # проходимся по списку тем и подтем\n",
        "    for ind, topic in tqdm(enumerate(list_topics)):\n",
        "        if ind%5 == 0:\n",
        "          with open(f'{self.save_project}{self.file_name[:20]}__loop{ind}.txt', \"w\") as f:\n",
        "              f.write(self.final_text)\n",
        "        # тему просто записываем в итоговый текст\n",
        "        if (\"_#\" in topic) and not(\"_#\" in list_topics[ind+1]):\n",
        "          self.final_text += f'\\n{topic}\\n'\n",
        "        # подтему передаем в ChatGPT\n",
        "        else:\n",
        "          self.final_text += f'\\n{topic}\\n'\n",
        "          self.organize_text(topic, num_chunks, model_topics)\n",
        "\n",
        "    with open(f'{self.save_project}{self.file_name[:20]}__final_text.txt', \"w\") as f:\n",
        "      f.write(self.final_text)\n",
        "    print(\"\\nСоздали финальный документ для Базы Знаний (final_text)\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "clear_output()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEB6lfh3vRYc"
      },
      "source": [
        "# Copywriter. Нейро-Копийтер для MarkdownHeaderTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Создаем текстовый файл с диалогами из Гугл таблицы"
      ],
      "metadata": {
        "id": "6BAWGeYk07T5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# функция для загрузки таблицы по ссылке из гугл драйв\n",
        "def load_xls_pd(url: str, sheet_name: str) -> str:\n",
        "    # Extract the document ID from the URL\n",
        "    match_ = re.search('/spreadsheets/d/([a-zA-Z0-9-_]+)', url)\n",
        "    if match_ is None:\n",
        "        raise ValueError('Invalid Google Sheets URL')\n",
        "    doc_id = match_.group(1)\n",
        "\n",
        "    # Download the table as pandas\n",
        "    response = requests.get(f'https://docs.google.com/spreadsheets/d/{doc_id}/export?format=xlsx')\n",
        "    response.raise_for_status()     #проверяет статус код ответа. Если получен ответ с кодом ошибки (4xx или 5xx), вызывается исключение HTTPError.\n",
        "    data = pd.read_excel(BytesIO(response.content), sheet_name=0)\n",
        "    #data = pd.read_excel(BytesIO(response.content),sheet_name='Sheet1') #чтение конкретного листа из Книги Excel\n",
        "\n",
        "    '''\n",
        "        Когда мы хотим прочитать данные Excel с помощью функции pd.read_excel(), она требует передачи ей пути к файлу или объекта, представляющего файл.\n",
        "        В параметре io функции pd.read_excel() необходимо указать путь к файлу (в виде строки, содержащей путь к файлу) или объект файлового типа (такой как BufferedWriter, BufferedReader и другие).\n",
        "        response.content возвращает содержимое ответа на запрос HTTP в виде байтового массива (bytes array).\n",
        "        Чтобы передать эти данные в функцию pd.read_excel(), нужно создать объект файла из байтового массива.\n",
        "        Для этой цели используется объект BytesIO из модуля io, который предоставляет интерфейс для работы с данными в памяти, как если бы они находились в файле.\n",
        "    '''\n",
        "    return data\n",
        "\n",
        "# Шаг 1: Загрузить xlsx файл из Google Drive\n",
        "google_sheet_url = \"https://docs.google.com/spreadsheets/d/165TBS6Fobxzx6rew29tzIDDe_4GuUDbRfUOpWNhew2E/edit?usp=sharing\"\n",
        "#sheet_name='Sheet1'\n",
        "data = load_xls_pd(google_sheet_url, sheet_name)\n",
        "\n",
        "# Шаг 2: Извлечь столбец \"text\"\n",
        "text_column = data[\"text\"]\n",
        "\n",
        "# Шаг 3: Заменить \"operatorMessage: Здравствуйте\" на \"<operatorMessage: Здравствуйте>\"\n",
        "text_column = text_column.str.replace(\"operatorMessage: Здравствуйте\", \"<operatorMessage: Здравствуйте>\")\n",
        "\n",
        "# Шаг 4: Сохранить в файл \"Dialogs__.txt\" в своей папке\n",
        "with open(\"/content/drive/MyDrive/Stazhirovka_Kia/Dialogs/Dialogs_Unable_to_determine/BASE/kia_dialog_Unable_to_determine.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for text in text_column:\n",
        "        f.write(str(text) + \"\\n\")\n",
        "\n",
        "print(\"Файл kia_dialog_____________.txt успешно сохранен!\")"
      ],
      "metadata": {
        "id": "RA_bKVkD1I5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYNb1d6q4hWX"
      },
      "outputs": [],
      "source": [
        "#@title Создаем объект для работы Copywriter\n",
        "projects_dir = '/content/drive/MyDrive/Stazhirovka_Kia/Dialogs/Dialogs_Unable_to_determine'\n",
        "\n",
        "Promt_copywriter = \"https://docs.google.com/document/d/1zePK0OWfOrBma99zfxxU6QzG2mu866vGpK4IwyNzcEw/edit?usp=sharing\"\n",
        "Promt_lector = \"https://docs.google.com/document/d/1eXZ27iOTuHtjtE4o2fWQlysZUudQ9-8rYi0eCGCupfM/edit?usp=sharing\"\n",
        "\n",
        "# если темы созданы\n",
        "# topics_final = \"/content/drive/MyDrive/Stazhirovka_Kia/Dialogs/___.txt\"\n",
        "# # Создаем объект для дообучения chatGPT\n",
        "curator = WorkerОpenAI(system_promt = Promt_copywriter, # системный промт\n",
        "                       system_promt_lector = Promt_lector, # промт Консультанта\n",
        "                      #  content_topics = topics_final,\n",
        "                       save_project = projects_dir)     # путь для сохранения готовых файлов\n",
        "\n",
        "# Формируем базу по файлу txt\n",
        "# # путь к материалам\n",
        "doc_txt_dir = projects_dir + '/BASE/'\n",
        "file_name = 'kia_dialog_Unable_to_determine.txt'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# curator.create_embedding_faiss_db(doc_txt_dir = doc_txt_dir,   # путь к материалам\n",
        "#                                 faiss_db_dir = db_initial,     # путь для сохранения исходной базы\n",
        "#                                 start_idx = 0,                 # номер документа в базе\n",
        "#                                 collection_name = 'УИИ_db_initial')  # наименование коллекции\n",
        "\n",
        "\n",
        "curator.create_embedding_one_file(doc_txt_dir = doc_txt_dir,   # путь к материалам\n",
        "                                  file_ = file_name,            # какой файл берем\n",
        "                                  faiss_db_dir = projects_dir)     # путь для сохранения исходной базы"
      ],
      "metadata": {
        "id": "qztY_BvINnUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGRXeHMLCfpI"
      },
      "outputs": [],
      "source": [
        "# Смотрим созданные чанки\n",
        "print(f\"Текст разбит на чанки. Всего: {len(curator.db.docstore._dict.values())} шт.\\n\")\n",
        "curator.db.docstore._dict.values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TLVJMHbJXPK"
      },
      "outputs": [],
      "source": [
        "#@title Поиск темы и подтемы выборкой по 5 - 15 чанков\n",
        "\n",
        "# Подаем в цикле и просим составить список тем и подтем.\n",
        "curator.search_topics_subtopics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbrAFSweUxC6"
      },
      "outputs": [],
      "source": [
        "#@title Корректируем список тем и подтем (обобщаем, убираем дубли).\n",
        "# Подаем сформированный Список тем и просим объединить похожие по смыслу темы или подтемы, записать компактно.\n",
        "# При необходимости перефразировать тему или подтему.\n",
        "curator.get_merge_topics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIm6znHAtWVL"
      },
      "source": [
        "## ОРГАНИЗУЕМ ИТОГОВЫЙ ТЕКСТ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fWybucTLHnm"
      },
      "source": [
        "Выборка блоков документов по схожести с подтемой. Передаем собранные отрывки с Лекции и просим рассказать подробнее, опираясь на Лекцию."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "projects_dir = '/content/drive/MyDrive/Stazhirovka_Kia/Dialogs/Dialogs_Unable_to_determine'\n",
        "\n",
        "Promt_copywriter = \"https://docs.google.com/document/d/1zePK0OWfOrBma99zfxxU6QzG2mu866vGpK4IwyNzcEw/edit?usp=sharing\"\n",
        "Promt_lector = \"https://docs.google.com/document/d/1zePK0OWfOrBma99zfxxU6QzG2mu866vGpK4IwyNzcEw/edit?usp=sharing\"\n",
        "\n",
        "# если темы созданы\n",
        "topics_final = \"/content/drive/MyDrive/Stazhirovka_Kia/Dialogs/Dialogs_Unable_to_determine/Dialogs_Unable_to_determinekia_dialog_Unable_to__темы_подтемы_итог.txt\"\n",
        "# # Создаем объект для дообучения chatGPT\n",
        "curator = WorkerОpenAI(system_promt = Promt_copywriter, # системный промт\n",
        "                       system_promt_lector = Promt_lector, # промт Лектора\n",
        "                       content_topics = topics_final,\n",
        "                       save_project = projects_dir)     # путь для сохранения готовых файлов\n"
      ],
      "metadata": {
        "id": "CqRs7TfFW-mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzRnco4xRX19"
      },
      "outputs": [],
      "source": [
        "# запускаем сборку Итогового Текста\n",
        "name_file = 'БЗ_kia_Dialogs_Unable_to_determine'\n",
        "db_initial = '/content/drive/MyDrive/Stazhirovka_Kia/Dialogs/Dialogs_Unable_to_determine/db_initial__kia_dialog_Unable_to_determine.'\n",
        "\n",
        "curator.organize_final_text(name = name_file,\n",
        "                            db_path = db_initial)\n",
        "\n",
        "# curator.organize_final_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iETHjZJRTnFC"
      },
      "outputs": [],
      "source": [
        "# сформирован итоговый текст\n",
        "curator.final_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOEPOxl_gQQW"
      },
      "source": [
        "# MarkdownHeaderTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRqMvP1XgQQY",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title функции\n",
        "chat_manager_system = \"\"\"\n",
        "Ты программист, специалист по Нейронным сетям, Профессионал Data Science и Преподаватель в Университете Искусственного Интеллекта.\n",
        "Ты Спикер на Вебинаре.\n",
        "Тебе предоставят отрывок с Вебинара и попросят раскрыть подробнее одну из тем.\n",
        "\n",
        "Твоя цель: Опираясь только на материал с Вебинара, подробно, развернуто рассказать по интересующей теме на русском языке.\n",
        "\"\"\"\n",
        "\n",
        "def get_chatgpt_answer(topic,  db, model = MODEL_TURBO_0613):\n",
        "  # Выборка документов по схожести с вопросом\n",
        "  docs = db.similarity_search(topic, k=4)\n",
        "  message_content = re.sub(r'\\n{2}', ' ', '\\n '.join([f'\\n  ' + doc.page_content + '\\n' for i, doc in enumerate(docs)]))\n",
        "  # print('message_content :\\n ======================================== \\n', self.message_content)\n",
        "\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": f\"{chat_manager_system}\"},\n",
        "    {\"role\": \"user\", \"content\": f\"\"\"Analyze step by step and give a detailed correct answer to the Student's question.\\n\n",
        "    Question:\\n{topic}\\n\\nMaterials from the Webinar:\\n{message_content}\\n\\nAnswer:\"\"\"}\n",
        "    ]\n",
        "\n",
        "  try:\n",
        "    completion = openai.ChatCompletion.create(\n",
        "    model=model,\n",
        "    messages=messages,\n",
        "    temperature=0.1\n",
        "    )\n",
        "\n",
        "    print(f'{completion[\"usage\"][\"total_tokens\"]} токенов использовано всего (вопрос-ответ).')\n",
        "    print('ЦЕНА запроса с ответом :', 0.0015*(completion[\"usage\"][\"total_tokens\"]/1000), ' $')\n",
        "    print('===========================================: \\n')\n",
        "    print('Ответ ChatGPT: ')\n",
        "    print(completion.choices[0].message.content)\n",
        "    # return completion.choices[0].message.content\n",
        "  except:\n",
        "    print(\"Модель в настоящее время перегружена. Попробуйте позже.\")\n",
        "\n",
        "def load_txt_file(file_path):\n",
        "    with open(file_path, 'r') as file_:\n",
        "        text = file_.read()\n",
        "    return text\n",
        "\n",
        "# projects_dir = '/content/drive/MyDrive/Colab Notebooks/_Projects_ChatGPT/Нейро_Copywriter/УИИ/'\n",
        "# name_file = '15_Разбор нейро-сотруд__final_text.txt'\n",
        "# final_text_dir = projects_dir + name_file\n",
        "\n",
        "# # Загружаем итоговый текст\n",
        "# final_text = load_txt_file(final_text_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbDRfWc_gQQY"
      },
      "outputs": [],
      "source": [
        "#@title  Готовим документ MarkdownHeader по сформированному тексту\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "\n",
        "headers_to_split_on = [\n",
        "    (\"_#\", \"Header 1\"),\n",
        "    (\"##_\", \"Header 2\"),\n",
        "    (\"###_\", \"Header 3\"),\n",
        "    (\"####_\", \"Header 4\"),\n",
        "    (\"#####_\", \"Header 5\"),\n",
        "]\n",
        "\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "md_header_splits = markdown_splitter.split_text(curator.final_text)\n",
        "md_header_splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqV2iXbJgQQZ"
      },
      "outputs": [],
      "source": [
        "file_name = 'Курс Нейро-струдники I Занятие 1 I 10.09. 2023_textAll.txt'\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1024, chunk_overlap=300\n",
        ")\n",
        "\n",
        "# Split\n",
        "split = text_splitter.split_documents(md_header_splits)\n",
        "db = FAISS.from_documents(split, OpenAIEmbeddings())\n",
        "db.save_local(os.path.join(projects_dir, f\"db_{file_name[:35]}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_3YhYPfgQQZ"
      },
      "outputs": [],
      "source": [
        "split[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQ4yUvHXgQQZ"
      },
      "outputs": [],
      "source": [
        "topic = \"\"\"\n",
        "Каких Нейросотрудников рассмотрим на курсе?\n",
        "\"\"\"\n",
        "\n",
        "get_chatgpt_answer(topic,  db)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GOEPOxl_gQQW"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}